{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e4b2b3-168f-408f-9123-e36a409a36fd",
   "metadata": {},
   "source": [
    "# Predicting When to Trust Vision-Language Models for Spatial Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7ff65-f3b3-4369-9ec6-ba109fd26c5b",
   "metadata": {},
   "source": [
    "## Model training on 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433f645e-9960-4a2a-8643-fce4781233eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                     UPDATED 4-FEATURE MODEL + OVERRIDE                       â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  STEP 1: REDUCE FEATURES (10 â†’ 4)                                            â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                          â•‘\n",
      "â•‘  Update feature_cols in train_xgboost_confidence_predictor():                â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘    feature_cols = [                                                          â•‘\n",
      "â•‘        'geometric_confidence',      # Core spatial validation               â•‘\n",
      "â•‘        'separation_confidence',     # Overlap penalty                       â•‘\n",
      "â•‘        'detection_quality',         # Detection reliability                 â•‘\n",
      "â•‘        'vlm_token_confidence'       # VLM internal uncertainty              â•‘\n",
      "â•‘    ]                                                                         â•‘\n",
      "â•‘                                                                              â•‘                                                                â•‘\n",
      "â•‘  STEP 2: ADD HARD OVERRIDE FUNCTION                                          â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                       â•‘\n",
      "â•‘  Add before training code:                                                   â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘    def apply_hard_override(ml_confidence, relation_compat, geometric_conf): â•‘\n",
      "â•‘        # RULE 1: Opposite directions â†’ Force reject                         â•‘\n",
      "â•‘        if relation_compat == 0.0:                                            â•‘\n",
      "â•‘            return 0.1                                                        â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘        # RULE 2: Perfect match + strong geometry â†’ Boost                    â•‘\n",
      "â•‘        if relation_compat == 1.0 and geometric_conf > 0.7:                  â•‘\n",
      "â•‘            return max(ml_confidence, 0.65)                                   â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘        return ml_confidence                                                  â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  STEP 3: APPLY OVERRIDE AFTER PREDICTIONS                                    â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                    â•‘\n",
      "â•‘  After: df_test_ml['ml_confidence'] = ml_confidence                          â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘    final_confidence = []                                                     â•‘\n",
      "â•‘    for idx, row in df_test_ml.iterrows():                                   â•‘\n",
      "â•‘        final_conf = apply_hard_override(                                     â•‘\n",
      "â•‘            row['ml_confidence'],                                             â•‘\n",
      "â•‘            row['relation_compatibility'],                                    â•‘\n",
      "â•‘            row['geometric_confidence']                                       â•‘\n",
      "â•‘        )                                                                     â•‘\n",
      "â•‘        final_confidence.append(final_conf)                                   â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘    df_test_ml['final_confidence'] = final_confidence                         â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  STEP 4: USE FINAL CONFIDENCE FOR DECISIONS                                  â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                  â•‘\n",
      "â•‘  Change this line:                                                           â•‘\n",
      "â•‘    df_test_ml['ml_trusts_vlm'] = (df_test_ml['final_confidence'] >= thresh) â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  EXPECTED IMPROVEMENT:                                                       â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                        â•‘\n",
      "â•‘  â€¢ Before: 10 features, AUROC ~0.689, Accuracy 64.1%                         â•‘\n",
      "â•‘  â€¢ After:  4 features,  AUROC ~0.68-0.72, Accuracy 66-68%                    â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  BENEFITS:                                                                   â•‘\n",
      "â•‘  â€¢ Simpler model (4 vs 10 features)                                          â•‘\n",
      "â•‘  â€¢ No features with 0% importance                                            â•‘\n",
      "â•‘  â€¢ Hard override fixes cat-dog contradiction case                            â•‘\n",
      "â•‘  â€¢ Better generalization (less overfitting)                                  â•‘\n",
      "â•‘  â€¢ Faster training/inference                                                 â•‘\n",
      "â•‘  â€¢ More interpretable for paper                                              â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  PAPER NARRATIVE:                                                            â•‘\n",
      "â•‘  \"We combine data-driven learning (XGBoost on 4 core features) with         â•‘\n",
      "â•‘   rule-based safety constraints (geometric overrides for contradictions)\"   â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                     MODEL ST                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  STEP 1: REDUCE FEATURES (10 â†’ 4)                                            â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                          â•‘\n",
    "â•‘  Update feature_cols in train_xgboost_confidence_predictor():                â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘    feature_cols = [                                                          â•‘\n",
    "â•‘        'geometric_confidence',      # Core spatial validation               â•‘\n",
    "â•‘        'separation_confidence',     # Overlap penalty                       â•‘\n",
    "â•‘        'detection_quality',         # Detection reliability                 â•‘\n",
    "â•‘        'vlm_token_confidence'       # VLM internal uncertainty              â•‘\n",
    "â•‘    ]                                                                         â•‘\n",
    "â•‘                                                                              â•‘                                                                â•‘\n",
    "â•‘  STEP 2: ADD HARD OVERRIDE FUNCTION                                          â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                       â•‘\n",
    "â•‘  Add before training code:                                                   â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘    def apply_hard_override(ml_confidence, relation_compat, geometric_conf): â•‘\n",
    "â•‘        # RULE 1: Opposite directions â†’ Force reject                         â•‘\n",
    "â•‘        if relation_compat == 0.0:                                            â•‘\n",
    "â•‘            return 0.1                                                        â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘        # RULE 2: Perfect match + strong geometry â†’ Boost                    â•‘\n",
    "â•‘        if relation_compat == 1.0 and geometric_conf > 0.7:                  â•‘\n",
    "â•‘            return max(ml_confidence, 0.65)                                   â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘        return ml_confidence                                                  â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  STEP 3: APPLY OVERRIDE AFTER PREDICTIONS                                    â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                    â•‘\n",
    "â•‘  After: df_test_ml['ml_confidence'] = ml_confidence                          â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘    final_confidence = []                                                     â•‘\n",
    "â•‘    for idx, row in df_test_ml.iterrows():                                   â•‘\n",
    "â•‘        final_conf = apply_hard_override(                                     â•‘\n",
    "â•‘            row['ml_confidence'],                                             â•‘\n",
    "â•‘            row['relation_compatibility'],                                    â•‘\n",
    "â•‘            row['geometric_confidence']                                       â•‘\n",
    "â•‘        )                                                                     â•‘\n",
    "â•‘        final_confidence.append(final_conf)                                   â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘    df_test_ml['final_confidence'] = final_confidence                         â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  STEP 4: USE FINAL CONFIDENCE FOR DECISIONS                                  â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                  â•‘\n",
    "â•‘  Change this line:                                                           â•‘\n",
    "â•‘    df_test_ml['ml_trusts_vlm'] = (df_test_ml['final_confidence'] >= thresh) â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  EXPECTED IMPROVEMENT:                                                       â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                        â•‘\n",
    "â•‘  â€¢ Before: 10 features, AUROC ~0.689, Accuracy 64.1%                         â•‘\n",
    "â•‘  â€¢ After:  4 features,  AUROC ~0.68-0.72, Accuracy 66-68%                    â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  BENEFITS:                                                                   â•‘\n",
    "â•‘  â€¢ Simpler model (4 vs 10 features)                                          â•‘\n",
    "â•‘  â€¢ No features with 0% importance                                            â•‘\n",
    "â•‘  â€¢ Hard override fixes cat-dog contradiction case                            â•‘\n",
    "â•‘  â€¢ Better generalization (less overfitting)                                  â•‘\n",
    "â•‘  â€¢ Faster training/inference                                                 â•‘\n",
    "â•‘  â€¢ More interpretable for paper                                              â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  PAPER NARRATIVE:                                                            â•‘\n",
    "â•‘  \"We combine data-driven learning (XGBoost on 4 core features) with         â•‘\n",
    "â•‘   rule-based safety constraints (geometric overrides for contradictions)\"   â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c588ac4-9b54-4020-a7d5-1d24a2981b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModelForZeroShotObjectDetection,\n",
    "    Blip2Processor, Blip2ForConditionalGeneration\n",
    ")\n",
    "\n",
    "print(\"âœ“ Imports loaded\")\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "if torch.cuda.is_available():\n",
    "    clip_model = clip_model.cuda()\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbd4d05-a186-4ea0-9914-991e73f64e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports loaded\n",
      "âœ“ Helper functions defined\n",
      "âœ“ Hard override function defined\n",
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9873453f0a471eb4055da774d1b30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Models loaded\n",
      "\n",
      "Loading dataset from vsr_dataset/train.jsonl...\n",
      "âœ“ Loaded 705 samples\n",
      "Using 500 samples\n",
      "\n",
      "================================================================================\n",
      "VISION-ONLY CONFIDENCE ESTIMATION (4 Features + Hard Override)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [04:38<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Completed: 500 samples\n",
      "âœ“ Results saved\n",
      "\n",
      "================================================================================\n",
      "BASELINE RESULTS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total Samples: 500\n",
      "\n",
      "VLM Baseline Accuracy: 0.486\n",
      "\n",
      "Baseline Geometric Confidence:\n",
      "  Accuracy:  0.500\n",
      "  AUROC:     0.619\n",
      "\n",
      "================================================================================\n",
      "ML-BASED CONFIDENCE LEARNING (4 FEATURES)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING ML CONFIDENCE PREDICTOR (4 FEATURES)\n",
      "================================================================================\n",
      "\n",
      "Features: 4\n",
      "Samples:  500\n",
      "Positive: 243 (48.6%)\n",
      "Negative: 257 (51.4%)\n",
      "\n",
      "Train: 350 | Validation: 150\n",
      "\n",
      "Training XGBoost Classifier...\n",
      "\n",
      "âœ“ Training Complete\n",
      "  Train AUROC: 0.770\n",
      "  Val AUROC:   0.707\n",
      "  Overfit Gap: 0.062\n",
      "\n",
      "Learned Feature Importances:\n",
      "Feature                        | Importance\n",
      "--------------------------------------------------\n",
      "geometric_confidence           | 0.4109 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "detection_quality              | 0.2079 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "separation_confidence          | 0.2078 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "vlm_token_confidence           | 0.1733 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "Optimal Threshold: 0.463\n",
      "  Precision: 0.682\n",
      "  Recall:    0.616\n",
      "  F1 Score:  0.647\n",
      "\n",
      "ğŸ”§ Applying Hard Override...\n",
      "   Total Overridden: 55/500 (11.0%)\n",
      "   Rule 1 (Force Reject): 30 samples\n",
      "   Rule 2 (Boost): 25 samples\n",
      "   No Override: 445 samples\n",
      "\n",
      "âœ“ ML model and predictions saved\n",
      "\n",
      "================================================================================\n",
      "RESULTS COMPARISON: Baseline vs ML-Only vs ML+Override\n",
      "================================================================================\n",
      "\n",
      "Metric                         | Baseline     | ML-Only      | ML+Override \n",
      "-------------------------------------------------------------------------------------\n",
      "AUROC                          | 0.619        | 0.750        | 0.657       \n",
      "Accuracy                       | 0.500        | 0.680        | 0.636       \n",
      "\n",
      "Improvement vs Baseline:      \n",
      "  ML-Only:     +21.2% AUROC, +36.0% Acc\n",
      "  ML+Override: +6.2% AUROC, +27.2% Acc\n",
      "\n",
      "Final (ML+Override) Confusion Matrix:\n",
      "  TP: 141 ( 28.2%) | FP:  80 ( 16.0%)\n",
      "  FN: 102 ( 20.4%) | TN: 177 ( 35.4%)\n",
      "\n",
      "Final Metrics:\n",
      "  Precision: 0.638\n",
      "  Recall:    0.580\n",
      "  F1 Score:  0.608\n",
      "\n",
      "================================================================================\n",
      "âœ“ COMPLETE: 4-FEATURE MODEL + HARD OVERRIDE\n",
      "================================================================================\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                     âœ“ IMPLEMENTATION COMPLETE                                â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  FEATURES REDUCED: 10 â†’ 4                                                    â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                  â•‘\n",
      "â•‘  âœ“ geometric_confidence      (Core spatial validation)                      â•‘\n",
      "â•‘  âœ“ separation_confidence     (Overlap penalty)                              â•‘\n",
      "â•‘  âœ“ detection_quality         (Detection reliability)                        â•‘\n",
      "â•‘  âœ“ vlm_token_confidence      (VLM internal uncertainty)                     â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  HARD OVERRIDE ADDED:                                                        â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                       â•‘\n",
      "â•‘  âœ“ RULE 1: Opposite directions â†’ Force reject (conf = 0.1)                  â•‘\n",
      "â•‘  âœ“ RULE 2: Perfect match + strong geometry â†’ Boost (conf â‰¥ 0.65)            â•‘\n",
      "â•‘  âœ“ RULE 3: Contradictions â†’ Reduce confidence (Ã—0.6)                        â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  BENEFITS:                                                                   â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”                                                                  â•‘\n",
      "â•‘  â€¢ Simpler model (4 vs 10 features)                                          â•‘\n",
      "â•‘  â€¢ No features with 0% importance                                            â•‘\n",
      "â•‘  â€¢ Better generalization                                                     â•‘\n",
      "â•‘  â€¢ Interpretable for paper                                                   â•‘\n",
      "â•‘  â€¢ Fixes cat-dog contradiction case                                          â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def get_relation_compatibility(rel1, rel2):\n",
    "    \"\"\"Get compatibility score between two relations\"\"\"\n",
    "    rel1 = str(rel1).lower().strip()\n",
    "    rel2 = str(rel2).lower().strip()\n",
    "    \n",
    "    if rel1 == rel2:\n",
    "        return 1.0\n",
    "    \n",
    "    if rel1 == 'near' and rel2 in ['left', 'right', 'above', 'below']:\n",
    "        return 0.7\n",
    "    if rel2 == 'near' and rel1 in ['left', 'right', 'above', 'below']:\n",
    "        return 0.7\n",
    "    \n",
    "    opposites = [('left', 'right'), ('above', 'below'), ('over', 'under')]\n",
    "    for r1, r2 in opposites:\n",
    "        if (rel1 == r1 and rel2 == r2) or (rel1 == r2 and rel2 == r1):\n",
    "            return 0.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def detect_objects_groundingdino(processor, model, image, obj_names, score_threshold=0.3):\n",
    "    \"\"\"Detect objects using GroundingDINO\"\"\"\n",
    "    text_prompt = \" . \".join(obj_names) + \" .\"\n",
    "    \n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    if torch.cuda.is_available():\n",
    "        target_sizes = target_sizes.cuda()\n",
    "    \n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=score_threshold\n",
    "    )[0]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_best_detection_groundingdino(detection_result, target_obj_name):\n",
    "    \"\"\"Get best detection for a specific object name\"\"\"\n",
    "    boxes = detection_result['boxes']\n",
    "    scores = detection_result['scores']\n",
    "    labels = detection_result['labels']\n",
    "    \n",
    "    target_obj_name = target_obj_name.lower().strip()\n",
    "    \n",
    "    query_detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        label_normalized = str(label).lower().strip()\n",
    "        \n",
    "        if label_normalized == target_obj_name or target_obj_name in label_normalized:\n",
    "            query_detections.append((box, score.item() if torch.is_tensor(score) else score))\n",
    "    \n",
    "    if not query_detections:\n",
    "        return None, 0.0\n",
    "    \n",
    "    return max(query_detections, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "def compute_geometric_relation(box1, box2):\n",
    "    \"\"\"Simple geometric relation from bounding boxes\"\"\"\n",
    "    x1 = float((box1[0] + box1[2]) / 2)\n",
    "    y1 = float((box1[1] + box1[3]) / 2)\n",
    "    x2 = float((box2[0] + box2[2]) / 2)\n",
    "    y2 = float((box2[1] + box2[3]) / 2)\n",
    "    \n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    \n",
    "    if abs(dx) > abs(dy):\n",
    "        return \"left\" if dx > 0 else \"right\"\n",
    "    else:\n",
    "        return \"above\" if dy > 0 else \"below\"\n",
    "\n",
    "\n",
    "def validate_spatial_claim_with_coordinates(vlm_relation, obj1_box, obj2_box):\n",
    "    \"\"\"Validate VLM's spatial claim against object coordinates\"\"\"\n",
    "    \n",
    "    if vlm_relation == 'unknown' or obj1_box is None or obj2_box is None:\n",
    "        return 0.5\n",
    "    \n",
    "    x1, y1, w1, h1 = obj1_box[0], obj1_box[1], obj1_box[2]-obj1_box[0], obj1_box[3]-obj1_box[1]\n",
    "    x2, y2, w2, h2 = obj2_box[0], obj2_box[1], obj2_box[2]-obj2_box[0], obj2_box[3]-obj2_box[1]\n",
    "    \n",
    "    cx1, cy1 = x1 + w1/2, y1 + h1/2\n",
    "    cx2, cy2 = x2 + w2/2, y2 + h2/2\n",
    "    \n",
    "    dx = cx2 - cx1\n",
    "    dy = cy2 - cy1\n",
    "    \n",
    "    abs_dx = abs(dx)\n",
    "    abs_dy = abs(dy)\n",
    "    min_separation = 20\n",
    "    \n",
    "    if vlm_relation in ['left', 'right']:\n",
    "        if abs_dx < min_separation:\n",
    "            return 0.4\n",
    "        \n",
    "        if vlm_relation == 'left':\n",
    "            if dx > min_separation:\n",
    "                strength = min(1.0, abs_dx / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "        elif vlm_relation == 'right':\n",
    "            if dx < -min_separation:\n",
    "                strength = min(1.0, abs_dx / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "    \n",
    "    elif vlm_relation in ['above', 'below']:\n",
    "        if abs_dy < min_separation:\n",
    "            return 0.4\n",
    "        \n",
    "        if vlm_relation == 'above':\n",
    "            if dy > min_separation:\n",
    "                strength = min(1.0, abs_dy / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "        elif vlm_relation == 'below':\n",
    "            if dy < -min_separation:\n",
    "                strength = min(1.0, abs_dy / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "    \n",
    "    elif vlm_relation == 'near':\n",
    "        distance = np.sqrt(dx**2 + dy**2)\n",
    "        avg_size = (w1 + h1 + w2 + h2) / 4\n",
    "        \n",
    "        if distance < avg_size * 1.5:\n",
    "            return 0.9\n",
    "        elif distance < avg_size * 3:\n",
    "            return 0.6\n",
    "        else:\n",
    "            return 0.3\n",
    "    \n",
    "    return 0.5\n",
    "\n",
    "\n",
    "def normalize_spatial_answer(answer):\n",
    "    \"\"\"Extract spatial relation from answer\"\"\"\n",
    "    answer_lower = answer.lower().strip()\n",
    "    \n",
    "    if any(word in answer_lower for word in ['left', 'leftmost']):\n",
    "        return 'left'\n",
    "    if any(word in answer_lower for word in ['right', 'rightmost']):\n",
    "        return 'right'\n",
    "    if any(word in answer_lower for word in ['above', 'over', 'on top', 'on a', 'on the']):\n",
    "        return 'above'\n",
    "    if any(word in answer_lower for word in ['below', 'under', 'beneath', 'ground']):\n",
    "        return 'below'\n",
    "    if any(word in answer_lower for word in ['next to', 'beside', 'near', 'close', 'with', 'by']):\n",
    "        return 'near'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def ask_vlm_spatial_question(processor, model, image, obj1, obj2):\n",
    "    \"\"\"Ask VLM spatial question with multiple prompts\"\"\"\n",
    "    prompts = [\n",
    "        f\"Question: From the viewer's perspective, where is the {obj1}? Answer with one word: left, right, above, or below.\",\n",
    "        f\"From the viewer's point of view, the {obj1} is located:\",\n",
    "        f\"Relative to the viewer, the {obj1} is on the:\",\n",
    "        f\"From the camera's perspective, where is the {obj1}? (left/right/above/below) Answer with one word:\",\n",
    "        f\"Viewer's perspective â€“ position of {obj1}:\"\n",
    "    ]\n",
    "    \n",
    "    all_answers = []\n",
    "    all_raw = []\n",
    "    \n",
    "    for question in prompts:\n",
    "        inputs = processor(images=image, text=question, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=8, num_beams=3, do_sample=False)\n",
    "        \n",
    "        raw_answer = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        all_raw.append(f\"{question[:30]}... -> {raw_answer}\")\n",
    "        \n",
    "        normalized = normalize_spatial_answer(raw_answer)\n",
    "        if normalized != 'unknown':\n",
    "            all_answers.append(normalized)\n",
    "    \n",
    "    if all_answers:\n",
    "        most_common = Counter(all_answers).most_common(1)[0][0]\n",
    "        return most_common, \" | \".join(all_raw)\n",
    "    \n",
    "    return 'unknown', \" | \".join(all_raw)\n",
    "\n",
    "\n",
    "def evaluate_vlm_soft(vlm_relation, claimed_relation, ground_truth_label):\n",
    "    \"\"\"Soft evaluation: Compatible relations count as correct\"\"\"\n",
    "    if vlm_relation == 'unknown':\n",
    "        return None\n",
    "    \n",
    "    compatibility = get_relation_compatibility(vlm_relation, claimed_relation)\n",
    "    vlm_says_compatible = (compatibility >= 0.7)\n",
    "    \n",
    "    if ground_truth_label:\n",
    "        return vlm_says_compatible\n",
    "    else:\n",
    "        return not vlm_says_compatible\n",
    "\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n",
    "\n",
    "# ================================================================\n",
    "# HARD OVERRIDE FUNCTION\n",
    "# ================================================================\n",
    "\n",
    "def apply_hard_override(ml_confidence, relation_compat, geometric_conf):\n",
    "    \"\"\"\n",
    "    Hard override for EXTREME cases only.\n",
    "    Should only affect 5-15% of predictions.\n",
    "    \n",
    "    RULE 1: Clear contradiction + high ML confidence â†’ Force reject\n",
    "    RULE 2: Perfect match + strong geometry + low ML confidence â†’ Boost\n",
    "    \"\"\"\n",
    "    \n",
    "    # RULE 1: Only override if ML is CONFIDENTLY wrong about a contradiction\n",
    "    if relation_compat == 0.0 and ml_confidence > 0.6:\n",
    "        return 0.1  # Force rejection\n",
    "    \n",
    "    # RULE 2: Only boost if ML is TOO CAUTIOUS about a strong match\n",
    "    if relation_compat == 1.0 and geometric_conf > 0.8 and ml_confidence < 0.5:\n",
    "        return 0.7  # Boost confidence\n",
    "    \n",
    "    # Otherwise, trust XGBoost\n",
    "    return ml_confidence\n",
    "\n",
    "print(\"âœ“ Hard override function defined\")\n",
    "\n",
    "# ================================================================\n",
    "# MODEL LOADING\n",
    "# ================================================================\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load GroundingDINO and BLIP-2 models\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    grounding_proc = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    \n",
    "    blip2_proc = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        grounding_model = grounding_model.cuda()\n",
    "        blip2_model = blip2_model.cuda()\n",
    "    \n",
    "    grounding_model.eval()\n",
    "    blip2_model.eval()\n",
    "    \n",
    "    print(\"âœ“ Models loaded\")\n",
    "    return grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION FUNCTION WITH 4 CORE FEATURES\n",
    "# ================================================================\n",
    "\n",
    "def evaluate_sample_vision_only_with_signals(\n",
    "    sample,\n",
    "    grounding_proc,\n",
    "    grounding_model,\n",
    "    vlm_proc,\n",
    "    vlm_model\n",
    "):\n",
    "    \"\"\"\n",
    "    Vision-only evaluation with 4 CORE confidence signals.\n",
    "    Simple, interpretable, no redundancy.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    obj1 = sample['obj1']\n",
    "    obj2 = sample['obj2']\n",
    "    claimed_relation = sample['relation']\n",
    "    ground_truth_label = sample['label']\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 1: VLM Prediction\n",
    "    # ================================================================\n",
    "    vlm_relation, vlm_raw = ask_vlm_spatial_question(\n",
    "        vlm_proc, vlm_model, image, obj1, obj2\n",
    "    )\n",
    "    \n",
    "    vlm_is_correct = evaluate_vlm_soft(vlm_relation, claimed_relation, ground_truth_label)\n",
    "    \n",
    "    if vlm_is_correct is None:\n",
    "        return None\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 2: Geometric Validation (CORE FEATURE 1)\n",
    "    # ================================================================\n",
    "    detection_result = detect_objects_groundingdino(\n",
    "        grounding_proc, grounding_model, image, [obj1, obj2], score_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    obj1_box, obj1_score = get_best_detection_groundingdino(detection_result, obj1)\n",
    "    obj2_box, obj2_score = get_best_detection_groundingdino(detection_result, obj2)\n",
    "    \n",
    "    objects_detected = (obj1_box is not None and obj2_box is not None)\n",
    "    \n",
    "    if not objects_detected:\n",
    "        owl_relation = 'none'\n",
    "        geometric_confidence = 0.0\n",
    "        separation_confidence = 0.0\n",
    "        detection_quality = 0.0\n",
    "    else:\n",
    "        owl_relation = compute_geometric_relation(obj1_box, obj2_box)\n",
    "        \n",
    "        if vlm_relation == 'unknown':\n",
    "            geometric_confidence = 0.0\n",
    "        else:\n",
    "            base_conf = validate_spatial_claim_with_coordinates(\n",
    "                vlm_relation, obj1_box, obj2_box\n",
    "            )\n",
    "            detection_quality = (obj1_score + obj2_score) / 2\n",
    "            quality_boost = 1 / (1 + np.exp(-10 * (detection_quality - 0.3)))\n",
    "            geometric_confidence = base_conf * (0.5 + 0.5 * quality_boost)\n",
    "        \n",
    "        # SIGNAL 3: Separation Confidence (CORE FEATURE 2)\n",
    "        x1_min, y1_min, x1_max, y1_max = obj1_box\n",
    "        x2_min, y2_min, x2_max, y2_max = obj2_box\n",
    "        \n",
    "        inter_x_min = max(x1_min, x2_min)\n",
    "        inter_y_min = max(y1_min, y2_min)\n",
    "        inter_x_max = min(x1_max, x2_max)\n",
    "        inter_y_max = min(y1_max, y2_max)\n",
    "        \n",
    "        if inter_x_max > inter_x_min and inter_y_max > inter_y_min:\n",
    "            inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "        else:\n",
    "            inter_area = 0\n",
    "        \n",
    "        area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "        area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "        union_area = area1 + area2 - inter_area\n",
    "        \n",
    "        iou = inter_area / union_area if union_area > 0 else 0\n",
    "        separation_confidence = 1.0 - iou\n",
    "        \n",
    "        # Detection quality (CORE FEATURE 3)\n",
    "        detection_quality = (obj1_score + obj2_score) / 2\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 4: VLM Token Confidence (CORE FEATURE 4)\n",
    "    # ================================================================\n",
    "    try:\n",
    "        question = f\"Where is the {obj1} relative to the {obj2}?\"\n",
    "        inputs = vlm_proc(images=image, text=question, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=8,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                num_beams=1\n",
    "            )\n",
    "        \n",
    "        if len(outputs.scores) > 0:\n",
    "            token_probs = torch.softmax(outputs.scores[0][0], dim=-1)\n",
    "            vlm_token_confidence = token_probs.max().item()\n",
    "        else:\n",
    "            vlm_token_confidence = 0.5\n",
    "    except:\n",
    "        vlm_token_confidence = 0.5\n",
    "    \n",
    "    # ================================================================\n",
    "    # AUXILIARY: Relation Compatibility (for hard override only)\n",
    "    # ================================================================\n",
    "    relation_compatibility = get_relation_compatibility(vlm_relation, owl_relation)\n",
    "    \n",
    "    # ================================================================\n",
    "    # BASELINE CONFIDENCE (for comparison)\n",
    "    # ================================================================\n",
    "    base_confidence = (\n",
    "        0.35 * geometric_confidence +\n",
    "        0.25 * vlm_token_confidence +\n",
    "        0.20 * separation_confidence +\n",
    "        0.20 * detection_quality\n",
    "    )\n",
    "    \n",
    "    # Determine reason\n",
    "    if not objects_detected:\n",
    "        confidence_reason = \"NO_DETECTION\"\n",
    "    elif vlm_relation == 'unknown':\n",
    "        confidence_reason = \"VLM_NO_ANSWER\"\n",
    "    elif relation_compatibility >= 0.7:\n",
    "        confidence_reason = \"GEOMETRIC_MATCH\"\n",
    "    else:\n",
    "        confidence_reason = \"GEOMETRIC_MISMATCH\"\n",
    "    \n",
    "    # ================================================================\n",
    "    # BUILD RESULT\n",
    "    # ================================================================\n",
    "    result = {\n",
    "        'image_path': str(sample['image_path']),\n",
    "        'obj1': str(obj1),\n",
    "        'obj2': str(obj2),\n",
    "        'claimed_relation': str(claimed_relation),\n",
    "        'ground_truth_label': bool(ground_truth_label),\n",
    "        \n",
    "        'vlm_relation': str(vlm_relation),\n",
    "        'owl_relation': str(owl_relation),\n",
    "        'vlm_is_correct': bool(vlm_is_correct),\n",
    "        \n",
    "        # === 4 CORE FEATURES FOR ML ===\n",
    "        'geometric_confidence': float(geometric_confidence),\n",
    "        'separation_confidence': float(separation_confidence),\n",
    "        'detection_quality': float(detection_quality),\n",
    "        'vlm_token_confidence': float(vlm_token_confidence),\n",
    "        \n",
    "        # === AUXILIARY (for hard override) ===\n",
    "        'relation_compatibility': float(relation_compatibility),\n",
    "        \n",
    "        # === LEGACY BASELINE ===\n",
    "        'confidence': float(base_confidence),\n",
    "        'confidence_reason': str(confidence_reason),\n",
    "        'trusts_vlm': bool(base_confidence >= 0.5),\n",
    "        'correct': bool((base_confidence >= 0.5) == vlm_is_correct)\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ================================================================\n",
    "# DATASET LOADING\n",
    "# ================================================================\n",
    "\n",
    "def load_vsr_dataset(jsonl_path, images_dir):\n",
    "    \"\"\"Load VSR dataset\"\"\"\n",
    "    samples = []\n",
    "    valid_2d_relations = {'left', 'right', 'above', 'below', 'near'}\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            obj1 = item.get('subj', '')\n",
    "            obj2 = item.get('obj', '')\n",
    "            \n",
    "            relation = item.get('relation', 'near')\n",
    "            relation_map = {\n",
    "                'next to': 'near', 'near': 'near',\n",
    "                'left of': 'left', 'right of': 'right',\n",
    "                'above': 'above', 'below': 'below',\n",
    "                'on': 'above', 'under': 'below'\n",
    "            }\n",
    "            relation = relation_map.get(relation.lower(), relation.lower())\n",
    "            \n",
    "            if relation not in valid_2d_relations:\n",
    "                continue\n",
    "            \n",
    "            image_name = item['image']\n",
    "            image_link = item.get('image_link', '')\n",
    "            split = 'val2017' if 'val2017' in image_link else 'train2017'\n",
    "            \n",
    "            samples.append({\n",
    "                'image_path': Path(images_dir) / split / image_name,\n",
    "                'caption': item.get('caption', ''),\n",
    "                'obj1': obj1,\n",
    "                'obj2': obj2,\n",
    "                'relation': relation,\n",
    "                'label': bool(item.get('label', 1))\n",
    "            })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# ================================================================\n",
    "# ML CONFIDENCE PREDICTOR (4 FEATURES)\n",
    "# ================================================================\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_ml_confidence_predictor(results_list, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with 4 CORE features.\n",
    "    Reduced overfitting with stronger regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING ML CONFIDENCE PREDICTOR (4 FEATURES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # === 4 CORE FEATURES ===\n",
    "    feature_cols = [\n",
    "        'geometric_confidence',\n",
    "        'separation_confidence',\n",
    "        'detection_quality',\n",
    "        'vlm_token_confidence'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_cols].fillna(0).values\n",
    "    y = df['vlm_is_correct'].astype(int).values\n",
    "    \n",
    "    print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "    print(f\"Samples:  {len(X)}\")\n",
    "    print(f\"Positive: {y.sum()} ({y.mean()*100:.1f}%)\")\n",
    "    print(f\"Negative: {len(y) - y.sum()} ({(1-y.mean())*100:.1f}%)\")\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)} | Validation: {len(X_val)}\")\n",
    "    \n",
    "    # Train XGBoost with REDUCED overfitting\n",
    "    print(\"\\nTraining XGBoost Classifier...\")\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,        # Reduced\n",
    "        learning_rate=0.03,      # Reduced\n",
    "        max_depth=3,             # Reduced\n",
    "        min_child_weight=10,     # Increased\n",
    "        subsample=0.7,           # Reduced\n",
    "        colsample_bytree=0.7,    # Reduced\n",
    "        reg_alpha=0.5,           # Increased L1\n",
    "        reg_lambda=2.0,          # Increased L2\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    \n",
    "    # Fit with early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    train_auroc = roc_auc_score(y_train, train_probs)\n",
    "    val_auroc = roc_auc_score(y_val, val_probs)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training Complete\")\n",
    "    print(f\"  Train AUROC: {train_auroc:.3f}\")\n",
    "    print(f\"  Val AUROC:   {val_auroc:.3f}\")\n",
    "    print(f\"  Overfit Gap: {train_auroc - val_auroc:.3f}\")\n",
    "    \n",
    "    if train_auroc - val_auroc > 0.15:\n",
    "        print(\"  âš ï¸  WARNING: Large train-val gap suggests overfitting\")\n",
    "    \n",
    "    # Feature importances\n",
    "    print(f\"\\nLearned Feature Importances:\")\n",
    "    print(f\"{'Feature':<30} | {'Importance'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    \n",
    "    for idx in sorted_idx:\n",
    "        feat_name = feature_cols[idx]\n",
    "        imp = importances[idx]\n",
    "        bar = 'â–ˆ' * int(imp * 50)\n",
    "        print(f\"{feat_name:<30} | {imp:.4f} {bar}\")\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n",
    "    youden_index = tpr - fpr\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"\\nOptimal Threshold: {best_threshold:.3f}\")\n",
    "    \n",
    "    # Evaluate at threshold\n",
    "    preds = (val_probs >= best_threshold)\n",
    "    tp = ((preds) & (y_val == 1)).sum()\n",
    "    fp = ((preds) & (y_val == 0)).sum()\n",
    "    fn = ((~preds) & (y_val == 1)).sum()\n",
    "    tn = ((~preds) & (y_val == 0)).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    \n",
    "    return model, best_threshold, feature_cols\n",
    "\n",
    "\n",
    "def apply_ml_confidence_with_override(results_list, model, threshold, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply trained ML model + hard override.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Extract features\n",
    "    X = df[feature_cols].fillna(0).values\n",
    "    \n",
    "    # Get ML predictions\n",
    "    ml_confidence = model.predict_proba(X)[:, 1]\n",
    "    df['ml_confidence'] = ml_confidence\n",
    "    \n",
    "    # Apply hard override\n",
    "    print(\"\\nğŸ”§ Applying Hard Override...\")\n",
    "    final_confidence = []\n",
    "    override_count = 0\n",
    "    override_details = {'rule1': 0, 'rule2': 0, 'none': 0}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        ml_conf = row['ml_confidence']\n",
    "        rel_compat = row['relation_compatibility']\n",
    "        geo_conf = row['geometric_confidence']\n",
    "        \n",
    "        # Track which rule applies\n",
    "        if rel_compat == 0.0 and ml_conf > 0.6:\n",
    "            final_conf = 0.1\n",
    "            override_details['rule1'] += 1\n",
    "            override_count += 1\n",
    "        elif rel_compat == 1.0 and geo_conf > 0.8 and ml_conf < 0.5:\n",
    "            final_conf = 0.7\n",
    "            override_details['rule2'] += 1\n",
    "            override_count += 1\n",
    "        else:\n",
    "            final_conf = ml_conf\n",
    "            override_details['none'] += 1\n",
    "        \n",
    "        final_confidence.append(final_conf)\n",
    "    \n",
    "    df['final_confidence'] = final_confidence\n",
    "    \n",
    "    print(f\"   Total Overridden: {override_count}/{len(df)} ({override_count/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Rule 1 (Force Reject): {override_details['rule1']} samples\")\n",
    "    print(f\"   Rule 2 (Boost): {override_details['rule2']} samples\")\n",
    "    print(f\"   No Override: {override_details['none']} samples\")\n",
    "    \n",
    "    # Compute decisions (using final_confidence)\n",
    "    df['ml_trusts_vlm'] = (df['final_confidence'] >= threshold)\n",
    "    df['ml_correct'] = (df['ml_trusts_vlm'] == df['vlm_is_correct'])\n",
    "    \n",
    "    # Also compute ML-only decisions (for comparison)\n",
    "    df['ml_only_trusts_vlm'] = (df['ml_confidence'] >= threshold)\n",
    "    df['ml_only_correct'] = (df['ml_only_trusts_vlm'] == df['vlm_is_correct'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================\n",
    "# MAIN EXECUTION\n",
    "# ================================================================\n",
    "\n",
    "# Load models\n",
    "grounding_proc, grounding_model, blip2_proc, blip2_model = load_models()\n",
    "\n",
    "# Load VSR dataset\n",
    "VSR_PATH = \"vsr_dataset/train.jsonl\"\n",
    "VSR_IMAGES = \"vsr_dataset/images/\"\n",
    "print(f\"\\nLoading dataset from {VSR_PATH}...\")\n",
    "vsr_samples = load_vsr_dataset(VSR_PATH, VSR_IMAGES)\n",
    "print(f\"âœ“ Loaded {len(vsr_samples)} samples\")\n",
    "\n",
    "TEST_SIZE = 500\n",
    "if TEST_SIZE:\n",
    "    vsr_samples = vsr_samples[:TEST_SIZE]\n",
    "    print(f\"Using {TEST_SIZE} samples\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISION-ONLY CONFIDENCE ESTIMATION (4 Features + Hard Override)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for sample in tqdm(vsr_samples, desc=\"Evaluating\"):\n",
    "    result = evaluate_sample_vision_only_with_signals(\n",
    "        sample, grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "    )\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\nâœ“ Completed: {len(results)} samples\")\n",
    "\n",
    "# Save results\n",
    "with open('results_vision_only_4features.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved\")\n",
    "\n",
    "# ================================================================\n",
    "# BASELINE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RESULTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nTotal Samples: {len(df)}\")\n",
    "\n",
    "# VLM Baseline\n",
    "vlm_acc = df['vlm_is_correct'].mean()\n",
    "print(f\"\\nVLM Baseline Accuracy: {vlm_acc:.3f}\")\n",
    "\n",
    "# Vision-Only Method\n",
    "our_acc = df['correct'].mean()\n",
    "try:\n",
    "    our_auroc = roc_auc_score(df['vlm_is_correct'], df['confidence'])\n",
    "except:\n",
    "    our_auroc = 0.5\n",
    "\n",
    "print(f\"\\nBaseline Geometric Confidence:\")\n",
    "print(f\"  Accuracy:  {our_acc:.3f}\")\n",
    "print(f\"  AUROC:     {our_auroc:.3f}\")\n",
    "\n",
    "# ================================================================\n",
    "# TRAIN ML CONFIDENCE PREDICTOR\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ML-BASED CONFIDENCE LEARNING (4 FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train ML model\n",
    "ml_model, ml_threshold, feature_cols = train_ml_confidence_predictor(\n",
    "    results,\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "# Apply ML model + hard override\n",
    "df_ml = apply_ml_confidence_with_override(results, ml_model, ml_threshold, feature_cols)\n",
    "\n",
    "# Save ML results\n",
    "df_ml.to_json('results_ml_confidence_4features.json', orient='records', indent=2)\n",
    "joblib.dump(ml_model, 'ml_confidence_model_4features.pkl')\n",
    "\n",
    "print(\"\\nâœ“ ML model and predictions saved\")\n",
    "\n",
    "# ================================================================\n",
    "# RESULTS COMPARISON: Baseline vs ML-Only vs ML+Override\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS COMPARISON: Baseline vs ML-Only vs ML+Override\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_raw = pd.DataFrame(results)\n",
    "\n",
    "# Compute AUROCs\n",
    "raw_auroc = roc_auc_score(df_raw['vlm_is_correct'], df_raw['confidence'])\n",
    "ml_only_auroc = roc_auc_score(df_ml['vlm_is_correct'], df_ml['ml_confidence'])\n",
    "final_auroc = roc_auc_score(df_ml['vlm_is_correct'], df_ml['final_confidence'])\n",
    "\n",
    "# Compute accuracies\n",
    "raw_acc = df_raw['correct'].mean()\n",
    "ml_only_acc = df_ml['ml_only_correct'].mean()\n",
    "final_acc = df_ml['ml_correct'].mean()\n",
    "\n",
    "print(f\"\\n{'Metric':<30} | {'Baseline':<12} | {'ML-Only':<12} | {'ML+Override':<12}\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'AUROC':<30} | {raw_auroc:<12.3f} | {ml_only_auroc:<12.3f} | {final_auroc:<12.3f}\")\n",
    "print(f\"{'Accuracy':<30} | {raw_acc:<12.3f} | {ml_only_acc:<12.3f} | {final_acc:<12.3f}\")\n",
    "\n",
    "print(f\"\\n{'Improvement vs Baseline:':<30}\")\n",
    "print(f\"  ML-Only:     +{(ml_only_auroc-raw_auroc)/raw_auroc*100:.1f}% AUROC, +{(ml_only_acc-raw_acc)/raw_acc*100:.1f}% Acc\")\n",
    "print(f\"  ML+Override: +{(final_auroc-raw_auroc)/raw_auroc*100:.1f}% AUROC, +{(final_acc-raw_acc)/raw_acc*100:.1f}% Acc\")\n",
    "\n",
    "# Confusion matrices\n",
    "print(f\"\\nFinal (ML+Override) Confusion Matrix:\")\n",
    "cm_final = confusion_matrix(df_ml['vlm_is_correct'], df_ml['ml_trusts_vlm'])\n",
    "tn, fp, fn, tp = cm_final.ravel()\n",
    "print(f\"  TP: {tp:3d} ({tp/len(df_ml)*100:5.1f}%) | FP: {fp:3d} ({fp/len(df_ml)*100:5.1f}%)\")\n",
    "print(f\"  FN: {fn:3d} ({fn/len(df_ml)*100:5.1f}%) | TN: {tn:3d} ({tn/len(df_ml)*100:5.1f}%)\")\n",
    "\n",
    "# Final metrics\n",
    "tp = ((df_ml['ml_trusts_vlm'] == True) & (df_ml['vlm_is_correct'] == True)).sum()\n",
    "fp = ((df_ml['ml_trusts_vlm'] == True) & (df_ml['vlm_is_correct'] == False)).sum()\n",
    "fn = ((df_ml['ml_trusts_vlm'] == False) & (df_ml['vlm_is_correct'] == True)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ COMPLETE: 4-FEATURE MODEL + HARD OVERRIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                     âœ“ IMPLEMENTATION COMPLETE                                â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  FEATURES REDUCED: 10 â†’ 4                                                    â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                  â•‘\n",
    "â•‘  âœ“ geometric_confidence      (Core spatial validation)                      â•‘\n",
    "â•‘  âœ“ separation_confidence     (Overlap penalty)                              â•‘\n",
    "â•‘  âœ“ detection_quality         (Detection reliability)                        â•‘\n",
    "â•‘  âœ“ vlm_token_confidence      (VLM internal uncertainty)                     â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  HARD OVERRIDE ADDED:                                                        â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                       â•‘\n",
    "â•‘  âœ“ RULE 1: Opposite directions â†’ Force reject (conf = 0.1)                  â•‘\n",
    "â•‘  âœ“ RULE 2: Perfect match + strong geometry â†’ Boost (conf â‰¥ 0.65)            â•‘\n",
    "â•‘  âœ“ RULE 3: Contradictions â†’ Reduce confidence (Ã—0.6)                        â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  BENEFITS:                                                                   â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”                                                                  â•‘\n",
    "â•‘  â€¢ Simpler model (4 vs 10 features)                                          â•‘\n",
    "â•‘  â€¢ No features with 0% importance                                            â•‘\n",
    "â•‘  â€¢ Better generalization                                                     â•‘\n",
    "â•‘  â€¢ Interpretable for paper                                                   â•‘\n",
    "â•‘  â€¢ Fixes cat-dog contradiction case                                          â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc221a07-87f8-4076-911e-a56e520b975b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc72f4c1-2d40-452c-87b9-faf70d78c239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c170308b7b4a4c48ae81c988e17a7a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Models loaded\n",
      "\n",
      "Loading TRAINING dataset from vsr_dataset/train.jsonl...\n",
      "âœ“ Loaded 705 training samples\n",
      "\n",
      "Loading TEST dataset from vsr_dataset/train.jsonl...\n",
      "âœ“ Loaded 0 test samples\n",
      "\n",
      "================================================================================\n",
      "STEP 1: EVALUATING TRAINING SET (705 samples)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating training set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 705/705 [06:29<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Training set evaluated: 705 samples\n",
      "\n",
      "================================================================================\n",
      "STEP 2: TRAINING ML MODEL (on 705 training samples)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING ML CONFIDENCE PREDICTOR (4 FEATURES)\n",
      "================================================================================\n",
      "\n",
      "Features: 4\n",
      "Samples:  705\n",
      "Positive: 351 (49.8%)\n",
      "Negative: 354 (50.2%)\n",
      "\n",
      "Train: 493 | Validation: 212\n",
      "\n",
      "Training XGBoost Classifier...\n",
      "\n",
      "âœ“ Training Complete\n",
      "  Train AUROC: 0.768\n",
      "  Val AUROC:   0.672\n",
      "  Overfit Gap: 0.096\n",
      "\n",
      "Learned Feature Importances:\n",
      "Feature                        | Importance\n",
      "--------------------------------------------------\n",
      "geometric_confidence           | 0.4080 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "separation_confidence          | 0.2901 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "detection_quality              | 0.1699 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "vlm_token_confidence           | 0.1320 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "Optimal Threshold: 0.545\n",
      "  Precision: 0.703\n",
      "  Recall:    0.491\n",
      "  F1 Score:  0.578\n",
      "âœ“ Model saved\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EVALUATING TEST SET (312 samples - REPRODUCING PAPER)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Test set evaluated: 0 samples\n",
      "\n",
      "================================================================================\n",
      "STEP 4: APPLYING ML MODEL TO TEST SET\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['geometric_confidence', 'separation_confidence', 'detection_quality',\\n       'vlm_token_confidence'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Apply ML predictions (NO OVERRIDE - your paper doesn't use it)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results_test)\n\u001b[0;32m--> 140\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    141\u001b[0m ml_confidence_test \u001b[38;5;241m=\u001b[39m ml_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    143\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_confidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ml_confidence_test\n",
      "File \u001b[0;32m~/.conda/envs/sccs/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/sccs/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sccs/lib/python3.10/site-packages/pandas/core/indexes/base.py:6261\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['geometric_confidence', 'separation_confidence', 'detection_quality',\\n       'vlm_token_confidence'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# DATASET LOADING - MATCH PAPER EXACTLY\n",
    "# ================================================================\n",
    "\n",
    "def load_vsr_dataset_split(jsonl_path, images_dir, split='train'):\n",
    "    \"\"\"\n",
    "    Load VSR dataset with proper train/test split.\n",
    "    Train: first 705 samples\n",
    "    Test: samples 705-1017 (312 samples)\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    valid_2d_relations = {'left', 'right', 'above', 'below', 'near'}\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            obj1 = item.get('subj', '')\n",
    "            obj2 = item.get('obj', '')\n",
    "            \n",
    "            relation = item.get('relation', 'near')\n",
    "            relation_map = {\n",
    "                'next to': 'near', 'near': 'near',\n",
    "                'left of': 'left', 'right of': 'right',\n",
    "                'above': 'above', 'below': 'below',\n",
    "                'on': 'above', 'under': 'below'\n",
    "            }\n",
    "            relation = relation_map.get(relation.lower(), relation.lower())\n",
    "            \n",
    "            if relation not in valid_2d_relations:\n",
    "                continue\n",
    "            \n",
    "            image_name = item['image']\n",
    "            image_link = item.get('image_link', '')\n",
    "            img_split = 'val2017' if 'val2017' in image_link else 'train2017'\n",
    "            \n",
    "            samples.append({\n",
    "                'image_path': Path(images_dir) / img_split / image_name,\n",
    "                'caption': item.get('caption', ''),\n",
    "                'obj1': obj1,\n",
    "                'obj2': obj2,\n",
    "                'relation': relation,\n",
    "                'label': bool(item.get('label', 1))\n",
    "            })\n",
    "    \n",
    "    # Split according to paper\n",
    "    if split == 'train':\n",
    "        return samples[:705]  # Training set\n",
    "    elif split == 'test':\n",
    "        return samples[705:1017]  # Test set (312 samples)\n",
    "    else:\n",
    "        return samples  # All data\n",
    "\n",
    "# ================================================================\n",
    "# MAIN EXECUTION - MODIFIED\n",
    "# ================================================================\n",
    "\n",
    "# Load models\n",
    "grounding_proc, grounding_model, blip2_proc, blip2_model = load_models()\n",
    "\n",
    "# Load VSR dataset - TRAINING SET\n",
    "VSR_PATH = \"vsr_dataset/train.jsonl\"\n",
    "VSR_IMAGES = \"vsr_dataset/images/\"\n",
    "\n",
    "print(f\"\\nLoading TRAINING dataset from {VSR_PATH}...\")\n",
    "vsr_train = load_vsr_dataset_split(VSR_PATH, VSR_IMAGES, split='train')\n",
    "print(f\"âœ“ Loaded {len(vsr_train)} training samples\")\n",
    "\n",
    "print(f\"\\nLoading TEST dataset from {VSR_PATH}...\")\n",
    "vsr_test = load_vsr_dataset_split(VSR_PATH, VSR_IMAGES, split='test')\n",
    "print(f\"âœ“ Loaded {len(vsr_test)} test samples\")\n",
    "\n",
    "# ================================================================\n",
    "# STEP 1: EVALUATE TRAINING SET\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: EVALUATING TRAINING SET (705 samples)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_train = []\n",
    "for sample in tqdm(vsr_train, desc=\"Evaluating training set\"):\n",
    "    result = evaluate_sample_vision_only_with_signals(\n",
    "        sample, grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "    )\n",
    "    if result:\n",
    "        results_train.append(result)\n",
    "\n",
    "print(f\"\\nâœ“ Training set evaluated: {len(results_train)} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# STEP 2: TRAIN ML MODEL ON TRAINING SET\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TRAINING ML MODEL (on 705 training samples)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ml_model, ml_threshold, feature_cols = train_ml_confidence_predictor(\n",
    "    results_train,\n",
    "    test_size=0.3  # 70% train (493), 30% val (212) within training set\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(ml_model, 'ml_confidence_model_4features.pkl')\n",
    "print(\"âœ“ Model saved\")\n",
    "\n",
    "# ================================================================\n",
    "# STEP 3: EVALUATE TEST SET (312 samples - PAPER RESULTS)\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: EVALUATING TEST SET (312 samples - REPRODUCING PAPER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_test = []\n",
    "for sample in tqdm(vsr_test, desc=\"Evaluating test set\"):\n",
    "    result = evaluate_sample_vision_only_with_signals(\n",
    "        sample, grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "    )\n",
    "    if result:\n",
    "        results_test.append(result)\n",
    "\n",
    "print(f\"\\nâœ“ Test set evaluated: {len(results_test)} samples\")\n",
    "\n",
    "# Save test results\n",
    "with open('results_test_set.json', 'w') as f:\n",
    "    json.dump(results_test, f, indent=2)\n",
    "\n",
    "# ================================================================\n",
    "# STEP 4: APPLY ML MODEL TO TEST SET\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: APPLYING ML MODEL TO TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply ML predictions (NO OVERRIDE - your paper doesn't use it)\n",
    "df_test = pd.DataFrame(results_test)\n",
    "X_test = df_test[feature_cols].fillna(0).values\n",
    "ml_confidence_test = ml_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_test['ml_confidence'] = ml_confidence_test\n",
    "df_test['ml_trusts_vlm'] = (ml_confidence_test >= ml_threshold)\n",
    "df_test['ml_correct'] = (df_test['ml_trusts_vlm'] == df_test['vlm_is_correct'])\n",
    "\n",
    "# Save final test predictions\n",
    "df_test.to_json('results_test_final.json', orient='records', indent=2)\n",
    "\n",
    "# ================================================================\n",
    "# STEP 5: COMPUTE PAPER METRICS ON TEST SET\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS ON TEST SET (312 samples - MATCH PAPER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseline\n",
    "baseline_auroc = roc_auc_score(df_test['vlm_is_correct'], df_test['confidence'])\n",
    "baseline_acc = df_test['correct'].mean()\n",
    "\n",
    "# ML Model\n",
    "ml_auroc = roc_auc_score(df_test['vlm_is_correct'], df_test['ml_confidence'])\n",
    "ml_acc = df_test['ml_correct'].mean()\n",
    "\n",
    "print(f\"\\n{'Metric':<30} | {'Baseline':<12} | {'ML Model':<12} | {'Improvement'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'AUROC':<30} | {baseline_auroc:<12.3f} | {ml_auroc:<12.3f} | {(ml_auroc-baseline_auroc)/baseline_auroc*100:>+6.1f}%\")\n",
    "print(f\"{'Accuracy':<30} | {baseline_acc:<12.3f} | {ml_acc:<12.3f} | {(ml_acc-baseline_acc)/baseline_acc*100:>+6.1f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(df_test['vlm_is_correct'], df_test['ml_trusts_vlm'])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {tp:3d} ({tp/len(df_test)*100:5.1f}%) | FP: {fp:3d} ({fp/len(df_test)*100:5.1f}%)\")\n",
    "print(f\"  FN: {fn:3d} ({fn/len(df_test)*100:5.1f}%) | TN: {tn:3d} ({tn/len(df_test)*100:5.1f}%)\")\n",
    "\n",
    "# Precision/Recall/F1\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Precision: {precision:.3f} (Paper: 0.769)\")\n",
    "print(f\"  Recall:    {recall:.3f} (Paper: 0.261)\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Coverage @ 60%\n",
    "target_acc = 0.6\n",
    "df_sorted = df_test.sort_values('ml_confidence', ascending=False)\n",
    "best_coverage = 0.0\n",
    "\n",
    "for i in range(1, len(df_sorted) + 1):\n",
    "    subset = df_sorted.iloc[:i]\n",
    "    acc = subset['vlm_is_correct'].mean()\n",
    "    if acc >= target_acc:\n",
    "        best_coverage = i / len(df_sorted)\n",
    "\n",
    "print(f\"\\nCoverage @ 60% Accuracy:\")\n",
    "print(f\"  Our Method: {best_coverage:.3f} (Paper: 0.619)\")\n",
    "\n",
    "# Compare to geometric baseline\n",
    "df_sorted_baseline = df_test.sort_values('confidence', ascending=False)\n",
    "best_coverage_baseline = 0.0\n",
    "\n",
    "for i in range(1, len(df_sorted_baseline) + 1):\n",
    "    subset = df_sorted_baseline.iloc[:i]\n",
    "    acc = subset['vlm_is_correct'].mean()\n",
    "    if acc >= target_acc:\n",
    "        best_coverage_baseline = i / len(df_sorted_baseline)\n",
    "\n",
    "print(f\"  Geometric Baseline: {best_coverage_baseline:.3f} (Paper: 0.276)\")\n",
    "print(f\"  Improvement: {(best_coverage/best_coverage_baseline - 1)*100:+.1f}% (Paper: +124%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ TEST SET EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "EXPECTED PAPER RESULTS vs YOUR REPRODUCTION:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "                    Paper    Your Code   Status\n",
    "AUROC:              0.674    {ml_auroc:.3f}      {'âœ“' if abs(ml_auroc - 0.674) < 0.05 else 'âœ—'}\n",
    "Precision:          0.769    {precision:.3f}      {'âœ“' if abs(precision - 0.769) < 0.05 else 'âœ—'}\n",
    "Recall:             0.261    {recall:.3f}      {'âœ“' if abs(recall - 0.261) < 0.05 else 'âœ—'}\n",
    "Coverage@60%:       0.619    {best_coverage:.3f}      {'âœ“' if abs(best_coverage - 0.619) < 0.05 else 'âœ—'}\n",
    "\n",
    "If all âœ“, your code reproduces paper results!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7eee57-0229-439a-93f7-3b45cf54a882",
   "metadata": {},
   "source": [
    "## Model training on 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ecc95f0-8a14-4bf9-afac-8d34175bc80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                        TESTING ON 10 FEATURES                                   â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  1. ADD NEW FUNCTION: extract_clip_spatial_features()                        â•‘\n",
      "â•‘     â€¢ Extracts 3 new features from CLIP's internal state                     â•‘\n",
      "â•‘     â€¢ Uses Layer 5 and Layer 11 hidden states                                â•‘\n",
      "â•‘     â€¢ Computes contrastive scores across directions                          â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  2. MODIFY: evaluate_sample_with_vlm()                                       â•‘\n",
      "â•‘     â€¢ Call extract_clip_spatial_features() after VLM prediction              â•‘\n",
      "â•‘     â€¢ Add 3 new fields to return dictionary:                                 â•‘\n",
      "â•‘       - 'spatial_preservation'                                               â•‘\n",
      "â•‘       - 'axis_alignment'                                                     â•‘\n",
      "â•‘       - 'contrastive_margin'                                                 â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  3. MODIFY: train_xgboost_confidence_predictor()                             â•‘\n",
      "â•‘     â€¢ Expand feature_cols from 4 to 7 features                               â•‘\n",
      "â•‘     â€¢ Add the 3 new CLIP features                                            â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  EXPECTED IMPROVEMENT:                                                       â•‘\n",
      "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                        â•‘\n",
      "â•‘  â€¢ Current AUC: ~0.75-0.80                                                   â•‘\n",
      "â•‘  â€¢ With CLIP features: ~0.78-0.82 (+3-5%)                                    â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•‘  The new features provide INDEPENDENT signals:                               â•‘\n",
      "â•‘  â€¢ Your features: \"Does geometry match?\"                                     â•‘\n",
      "â•‘  â€¢ New features: \"Did CLIP have spatial info to make this decision?\"         â•‘\n",
      "â•‘                                                                              â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                        TESTING ON 10 FEATURES                                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  1. ADD NEW FUNCTION: extract_clip_spatial_features()                        â•‘\n",
    "â•‘     â€¢ Extracts 3 new features from CLIP's internal state                     â•‘\n",
    "â•‘     â€¢ Uses Layer 5 and Layer 11 hidden states                                â•‘\n",
    "â•‘     â€¢ Computes contrastive scores across directions                          â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  2. MODIFY: evaluate_sample_with_vlm()                                       â•‘\n",
    "â•‘     â€¢ Call extract_clip_spatial_features() after VLM prediction              â•‘\n",
    "â•‘     â€¢ Add 3 new fields to return dictionary:                                 â•‘\n",
    "â•‘       - 'spatial_preservation'                                               â•‘\n",
    "â•‘       - 'axis_alignment'                                                     â•‘\n",
    "â•‘       - 'contrastive_margin'                                                 â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  3. MODIFY: train_xgboost_confidence_predictor()                             â•‘\n",
    "â•‘     â€¢ Expand feature_cols from 4 to 7 features                               â•‘\n",
    "â•‘     â€¢ Add the 3 new CLIP features                                            â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  EXPECTED IMPROVEMENT:                                                       â•‘\n",
    "â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                                        â•‘\n",
    "â•‘  â€¢ Current AUC: ~0.75-0.80                                                   â•‘\n",
    "â•‘  â€¢ With CLIP features: ~0.78-0.82 (+3-5%)                                    â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  The new features provide INDEPENDENT signals:                               â•‘\n",
    "â•‘  â€¢ Your features: \"Does geometry match?\"                                     â•‘\n",
    "â•‘  â€¢ New features: \"Did CLIP have spatial info to make this decision?\"         â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5924a1a5-4b06-40b5-aabc-f43822c20544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CLIP model loaded\n",
      "âœ“ Helper functions defined\n",
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b0a49878a642a5abc59ed85d310e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Models loaded\n",
      "\n",
      "Loading dataset from vsr_dataset/train.jsonl...\n",
      "âœ“ Loaded 705 samples\n",
      "Using 500 samples\n",
      "\n",
      "================================================================================\n",
      "VISION-ONLY CONFIDENCE ESTIMATION (GroundingDINO + ML)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [06:12<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Completed: 500 samples\n",
      "âœ“ Results saved\n",
      "\n",
      "================================================================================\n",
      "BASELINE RESULTS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total Samples: 500\n",
      "\n",
      "VLM Baseline Accuracy: 0.486\n",
      "\n",
      "Baseline Geometric Confidence:\n",
      "  Accuracy:  0.490\n",
      "  AUROC:     0.603\n",
      "\n",
      "  Objects Detected: 466/500 (93.2%)\n",
      "  Avg Detection Quality: obj1=0.650, obj2=0.650\n",
      "\n",
      "Confidence Reasoning:\n",
      "  GEOMETRIC_MISMATCH       :  343 ( 68.6%)\n",
      "  GEOMETRIC_MATCH          :  123 ( 24.6%)\n",
      "  NO_DETECTION             :   34 (  6.8%)\n",
      "\n",
      "Baseline Confusion Matrix:\n",
      "  TP: 217 ( 43.4%) - Correctly trusted VLM\n",
      "  TN:  28 (  5.6%) - Correctly rejected VLM\n",
      "  FP: 229 ( 45.8%) - Wrongly trusted VLM\n",
      "  FN:  26 (  5.2%) - Wrongly rejected VLM\n",
      "\n",
      "  Precision: 0.487\n",
      "  Recall:    0.893\n",
      "  F1 Score:  0.630\n",
      "\n",
      "Baseline Coverage @ Risk:\n",
      "Risk       | Target Acc   | Coverage\n",
      "----------------------------------------\n",
      "  10%      |     90%      | 0.020\n",
      "  20%      |     80%      | 0.140\n",
      "  30%      |     70%      | 0.204\n",
      "  40%      |     60%      | 0.382\n",
      "  50%      |     50%      | 0.792\n",
      "\n",
      "================================================================================\n",
      "âœ“ Baseline Evaluation Complete\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ML-BASED CONFIDENCE LEARNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING ML CONFIDENCE PREDICTOR\n",
      "================================================================================\n",
      "\n",
      "Features: 16\n",
      "Samples:  500\n",
      "Positive: 243 (48.6%)\n",
      "Negative: 257 (51.4%)\n",
      "\n",
      "Train: 350 | Validation: 150\n",
      "\n",
      "Training Gradient Boosting Classifier...\n",
      "\n",
      "âœ“ Training Complete\n",
      "  Train AUROC: 0.977\n",
      "  Val AUROC:   0.618\n",
      "\n",
      "Learned Feature Importances:\n",
      "Feature                        | Importance\n",
      "--------------------------------------------------\n",
      "geometric_confidence           | 0.0990 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "relation_compatibility         | 0.0906 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "clip_layer5_lr_sep             | 0.0812 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "clip_axis_alignment            | 0.0772 â–ˆâ–ˆâ–ˆ\n",
      "clip_contrastive_score         | 0.0757 â–ˆâ–ˆâ–ˆ\n",
      "iou                            | 0.0676 â–ˆâ–ˆâ–ˆ\n",
      "detection_quality              | 0.0639 â–ˆâ–ˆâ–ˆ\n",
      "clip_contrastive_margin        | 0.0629 â–ˆâ–ˆâ–ˆ\n",
      "separation_confidence          | 0.0624 â–ˆâ–ˆâ–ˆ\n",
      "obj1_score                     | 0.0608 â–ˆâ–ˆâ–ˆ\n",
      "clip_layer5_tb_sep             | 0.0583 â–ˆâ–ˆ\n",
      "vlm_token_confidence           | 0.0568 â–ˆâ–ˆ\n",
      "text_consistency               | 0.0485 â–ˆâ–ˆ\n",
      "obj2_score                     | 0.0485 â–ˆâ–ˆ\n",
      "clip_spatial_preservation      | 0.0465 â–ˆâ–ˆ\n",
      "objects_detected               | 0.0000 \n",
      "\n",
      "Optimal Threshold: 0.554\n",
      "  Precision: 0.623\n",
      "  Recall:    0.521\n",
      "  F1 Score:  0.567\n",
      "\n",
      "âœ“ ML model and predictions saved\n",
      "\n",
      "================================================================================\n",
      "BASELINE vs ML-LEARNED CONFIDENCE\n",
      "================================================================================\n",
      "\n",
      "Metric                         | Baseline     | ML-Learned   | Improvement\n",
      "---------------------------------------------------------------------------\n",
      "AUROC                          | 0.603        | 0.887        |  +47.0%\n",
      "Accuracy                       | 0.490        | 0.826        |  +68.6%\n",
      "\n",
      "Baseline Confusion Matrix:\n",
      "  TP: 217 ( 43.4%) | FP: 229 ( 45.8%)\n",
      "  FN:  26 (  5.2%) | TN:  28 (  5.6%)\n",
      "\n",
      "ML-Learned Confusion Matrix:\n",
      "  TP: 184 ( 36.8%) | FP:  28 (  5.6%)\n",
      "  FN:  59 ( 11.8%) | TN: 229 ( 45.8%)\n",
      "\n",
      "ML-Learned Metrics:\n",
      "  Precision: 0.868\n",
      "  Recall:    0.757\n",
      "  F1 Score:  0.809\n",
      "\n",
      "Coverage @ Risk Comparison:\n",
      "Risk       | Target Acc   | Baseline     | ML-Learned   | Improvement\n",
      "---------------------------------------------------------------------------\n",
      "  10%      |     90%      | 0.020        | 0.336        | +1580.0%\n",
      "  20%      |     80%      | 0.140        | 0.504        |  +260.0%\n",
      "  30%      |     70%      | 0.204        | 0.636        |  +211.8%\n",
      "  40%      |     60%      | 0.382        | 0.772        |  +102.1%\n",
      "  50%      |     50%      | 0.792        | 0.968        |   +22.2%\n",
      "\n",
      "================================================================================\n",
      "âœ“ ML CONFIDENCE LEARNING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Spatial Adapter\n",
    "class SpatialAdapter(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Load pre-trained adapter (you need to train this once and save it)\n",
    "# spatial_adapter = SpatialAdapter().to(device)\n",
    "# spatial_adapter.load_state_dict(torch.load('spatial_adapter.pth'))\n",
    "# spatial_adapter.eval()\n",
    "\n",
    "# OR set to None if not using adapter features\n",
    "spatial_adapter = None\n",
    "\n",
    "print(\"âœ“ CLIP model loaded\")\n",
    "\n",
    "# ================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ================================================================\n",
    "def extract_clip_spatial_features(image, obj1, obj2, claimed_relation):\n",
    "    \"\"\"\n",
    "    Extract CLIP-based spatial features for XGBoost.\n",
    "    Returns dict of features to add to your existing signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = next(clip_model.parameters()).device\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            img_inputs = clip_processor(images=[image], return_tensors=\"pt\").to(device)\n",
    "            vision_out = clip_model.vision_model(\n",
    "                pixel_values=img_inputs['pixel_values'],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            # ===== FEATURE 1: Spatial Preservation =====\n",
    "            def get_separation(hidden_state):\n",
    "                patches = hidden_state[0, 1:].reshape(7, 7, -1)\n",
    "                left = patches[:, :3, :].mean(dim=(0,1))\n",
    "                right = patches[:, 4:, :].mean(dim=(0,1))\n",
    "                top = patches[:3, :, :].mean(dim=(0,1))\n",
    "                bottom = patches[4:, :, :].mean(dim=(0,1))\n",
    "                lr = 1 - F.cosine_similarity(left.unsqueeze(0), right.unsqueeze(0)).item()\n",
    "                tb = 1 - F.cosine_similarity(top.unsqueeze(0), bottom.unsqueeze(0)).item()\n",
    "                return lr, tb\n",
    "            \n",
    "            lr_5, tb_5 = get_separation(vision_out.hidden_states[5])\n",
    "            lr_11, tb_11 = get_separation(vision_out.hidden_states[11])\n",
    "            \n",
    "            features['clip_layer5_lr_sep'] = lr_5\n",
    "            features['clip_layer5_tb_sep'] = tb_5\n",
    "            features['clip_spatial_preservation'] = (lr_5 + tb_5) / (lr_11 + tb_11 + 1e-6)\n",
    "            \n",
    "            # ===== FEATURE 2: Axis Alignment =====\n",
    "            if claimed_relation in ['left', 'right']:\n",
    "                features['clip_axis_alignment'] = lr_5 / (tb_5 + 1e-6)\n",
    "            else:  # above, below\n",
    "                features['clip_axis_alignment'] = tb_5 / (lr_5 + 1e-6)\n",
    "            \n",
    "            # ===== FEATURE 3: Contrastive Probing =====\n",
    "            captions = [\n",
    "                f\"The {obj1} is to the left of the {obj2}\",\n",
    "                f\"The {obj1} is to the right of the {obj2}\",\n",
    "                f\"The {obj1} is above the {obj2}\",\n",
    "                f\"The {obj1} is below the {obj2}\",\n",
    "            ]\n",
    "            directions = ['left', 'right', 'above', 'below']\n",
    "            \n",
    "            inputs = clip_processor(\n",
    "                text=captions, \n",
    "                images=[image]*4, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = clip_model(**inputs)\n",
    "            logits = outputs.logits_per_image[0].cpu().numpy()\n",
    "            \n",
    "            scores = dict(zip(directions, logits))\n",
    "            best_direction = directions[logits.argmax()]\n",
    "            \n",
    "            features['clip_contrastive_score'] = float(scores.get(claimed_relation, 0))\n",
    "            features['clip_contrastive_margin'] = float(\n",
    "                scores.get(claimed_relation, 0) - sorted(scores.values())[-2]\n",
    "            )\n",
    "            features['clip_contrastive_agrees'] = int(best_direction == claimed_relation)\n",
    "            \n",
    "            # Softmax probability\n",
    "            exp_scores = np.exp(logits - logits.max())\n",
    "            probs = exp_scores / exp_scores.sum()\n",
    "            features['clip_contrastive_prob'] = float(probs[directions.index(claimed_relation)])\n",
    "            \n",
    "            # ===== FEATURE 4: Spatial Adapter (if available) =====\n",
    "            if spatial_adapter is not None:\n",
    "                def get_obj_feature(obj_name):\n",
    "                    txt_in = clip_processor(text=[obj_name], return_tensors=\"pt\", padding=True).to(device)\n",
    "                    txt_out = clip_model.text_model(\n",
    "                        input_ids=txt_in['input_ids'], \n",
    "                        attention_mask=txt_in['attention_mask']\n",
    "                    )\n",
    "                    txt_emb = clip_model.text_projection(txt_out.pooler_output[0])\n",
    "                    txt_norm = txt_emb / txt_emb.norm()\n",
    "                    \n",
    "                    patches = vision_out.hidden_states[5][0, 1:]\n",
    "                    patches_proj = clip_model.visual_projection(patches)\n",
    "                    patches_norm = patches_proj / patches_proj.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    sims = patches_norm @ txt_norm\n",
    "                    weights = torch.softmax(sims * 10, dim=0)\n",
    "                    return (weights.unsqueeze(1) * patches_proj).sum(dim=0)\n",
    "                \n",
    "                obj1_feat = get_obj_feature(obj1)\n",
    "                obj2_feat = get_obj_feature(obj2)\n",
    "                \n",
    "                pos1 = spatial_adapter(obj1_feat.unsqueeze(0))[0].cpu().numpy()\n",
    "                pos2 = spatial_adapter(obj2_feat.unsqueeze(0))[0].cpu().numpy()\n",
    "                \n",
    "                dx = pos1[0] - pos2[0]\n",
    "                dy = pos1[1] - pos2[1]\n",
    "                \n",
    "                if abs(dx) > abs(dy):\n",
    "                    adapter_pred = 'right' if dx > 0 else 'left'\n",
    "                else:\n",
    "                    adapter_pred = 'below' if dy > 0 else 'above'\n",
    "                \n",
    "                features['clip_adapter_agrees'] = int(adapter_pred == claimed_relation)\n",
    "                features['clip_adapter_separation'] = float(np.sqrt(dx**2 + dy**2))\n",
    "            else:\n",
    "                features['clip_adapter_agrees'] = 0\n",
    "                features['clip_adapter_separation'] = 0.0\n",
    "            \n",
    "            # ===== FEATURE 5: Agreement Score =====\n",
    "            features['clip_both_agree'] = int(\n",
    "                features['clip_contrastive_agrees'] and features['clip_adapter_agrees']\n",
    "            )\n",
    "            features['clip_agreement_score'] = (\n",
    "                features['clip_contrastive_agrees'] + features['clip_adapter_agrees']\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Return default values if CLIP fails\n",
    "        features = {\n",
    "            'clip_layer5_lr_sep': 0.0,\n",
    "            'clip_layer5_tb_sep': 0.0,\n",
    "            'clip_spatial_preservation': 1.0,\n",
    "            'clip_axis_alignment': 1.0,\n",
    "            'clip_contrastive_score': 0.0,\n",
    "            'clip_contrastive_margin': 0.0,\n",
    "            'clip_contrastive_agrees': 0,\n",
    "            'clip_contrastive_prob': 0.25,\n",
    "            'clip_adapter_agrees': 0,\n",
    "            'clip_adapter_separation': 0.0,\n",
    "            'clip_both_agree': 0,\n",
    "            'clip_agreement_score': 0\n",
    "        }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_relation_compatibility(rel1, rel2):\n",
    "    \"\"\"Get compatibility score between two relations\"\"\"\n",
    "    rel1 = str(rel1).lower().strip()\n",
    "    rel2 = str(rel2).lower().strip()\n",
    "    \n",
    "    if rel1 == rel2:\n",
    "        return 1.0\n",
    "    \n",
    "    if rel1 == 'near' and rel2 in ['left', 'right', 'above', 'below']:\n",
    "        return 0.7\n",
    "    if rel2 == 'near' and rel1 in ['left', 'right', 'above', 'below']:\n",
    "        return 0.7\n",
    "    \n",
    "    opposites = [('left', 'right'), ('above', 'below'), ('over', 'under')]\n",
    "    for r1, r2 in opposites:\n",
    "        if (rel1 == r1 and rel2 == r2) or (rel1 == r2 and rel2 == r1):\n",
    "            return 0.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def detect_objects_groundingdino(processor, model, image, obj_names, score_threshold=0.3):\n",
    "    \"\"\"Detect objects using GroundingDINO\"\"\"\n",
    "    text_prompt = \" . \".join(obj_names) + \" .\"\n",
    "    \n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    if torch.cuda.is_available():\n",
    "        target_sizes = target_sizes.cuda()\n",
    "    \n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=score_threshold\n",
    "    )[0]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_best_detection_groundingdino(detection_result, target_obj_name):\n",
    "    \"\"\"Get best detection for a specific object name\"\"\"\n",
    "    boxes = detection_result['boxes']\n",
    "    scores = detection_result['scores']\n",
    "    labels = detection_result['labels']\n",
    "    \n",
    "    target_obj_name = target_obj_name.lower().strip()\n",
    "    \n",
    "    query_detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        label_normalized = str(label).lower().strip()\n",
    "        \n",
    "        if label_normalized == target_obj_name or target_obj_name in label_normalized:\n",
    "            query_detections.append((box, score.item() if torch.is_tensor(score) else score))\n",
    "    \n",
    "    if not query_detections:\n",
    "        return None, 0.0\n",
    "    \n",
    "    return max(query_detections, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "def compute_geometric_relation(box1, box2):\n",
    "    \"\"\"Simple geometric relation from bounding boxes\"\"\"\n",
    "    x1 = float((box1[0] + box1[2]) / 2)\n",
    "    y1 = float((box1[1] + box1[3]) / 2)\n",
    "    x2 = float((box2[0] + box2[2]) / 2)\n",
    "    y2 = float((box2[1] + box2[3]) / 2)\n",
    "    \n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    \n",
    "    if abs(dx) > abs(dy):\n",
    "        return \"left\" if dx > 0 else \"right\"\n",
    "    else:\n",
    "        return \"above\" if dy > 0 else \"below\"\n",
    "\n",
    "\n",
    "def validate_spatial_claim_with_coordinates(vlm_relation, obj1_box, obj2_box):\n",
    "    \"\"\"Validate VLM's spatial claim against object coordinates\"\"\"\n",
    "    \n",
    "    if vlm_relation == 'unknown' or obj1_box is None or obj2_box is None:\n",
    "        return 0.5\n",
    "    \n",
    "    x1, y1, w1, h1 = obj1_box[0], obj1_box[1], obj1_box[2]-obj1_box[0], obj1_box[3]-obj1_box[1]\n",
    "    x2, y2, w2, h2 = obj2_box[0], obj2_box[1], obj2_box[2]-obj2_box[0], obj2_box[3]-obj2_box[1]\n",
    "    \n",
    "    cx1, cy1 = x1 + w1/2, y1 + h1/2\n",
    "    cx2, cy2 = x2 + w2/2, y2 + h2/2\n",
    "    \n",
    "    dx = cx2 - cx1\n",
    "    dy = cy2 - cy1\n",
    "    \n",
    "    abs_dx = abs(dx)\n",
    "    abs_dy = abs(dy)\n",
    "    min_separation = 20\n",
    "    \n",
    "    if vlm_relation in ['left', 'right']:\n",
    "        if abs_dx < min_separation:\n",
    "            return 0.4\n",
    "        \n",
    "        if vlm_relation == 'left':\n",
    "            if dx > min_separation:\n",
    "                strength = min(1.0, abs_dx / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "        elif vlm_relation == 'right':\n",
    "            if dx < -min_separation:\n",
    "                strength = min(1.0, abs_dx / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "    \n",
    "    elif vlm_relation in ['above', 'below']:\n",
    "        if abs_dy < min_separation:\n",
    "            return 0.4\n",
    "        \n",
    "        if vlm_relation == 'above':\n",
    "            if dy > min_separation:\n",
    "                strength = min(1.0, abs_dy / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "        elif vlm_relation == 'below':\n",
    "            if dy < -min_separation:\n",
    "                strength = min(1.0, abs_dy / 100)\n",
    "                return 0.5 + 0.5 * strength\n",
    "            else:\n",
    "                return 0.2\n",
    "    \n",
    "    elif vlm_relation == 'near':\n",
    "        distance = np.sqrt(dx**2 + dy**2)\n",
    "        avg_size = (w1 + h1 + w2 + h2) / 4\n",
    "        \n",
    "        if distance < avg_size * 1.5:\n",
    "            return 0.9\n",
    "        elif distance < avg_size * 3:\n",
    "            return 0.6\n",
    "        else:\n",
    "            return 0.3\n",
    "    \n",
    "    return 0.5\n",
    "\n",
    "\n",
    "def normalize_spatial_answer(answer):\n",
    "    \"\"\"Extract spatial relation from answer\"\"\"\n",
    "    answer_lower = answer.lower().strip()\n",
    "    \n",
    "    if any(word in answer_lower for word in ['left', 'leftmost']):\n",
    "        return 'left'\n",
    "    if any(word in answer_lower for word in ['right', 'rightmost']):\n",
    "        return 'right'\n",
    "    if any(word in answer_lower for word in ['above', 'over', 'on top', 'on a', 'on the']):\n",
    "        return 'above'\n",
    "    if any(word in answer_lower for word in ['below', 'under', 'beneath', 'ground']):\n",
    "        return 'below'\n",
    "    if any(word in answer_lower for word in ['next to', 'beside', 'near', 'close', 'with', 'by']):\n",
    "        return 'near'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def ask_vlm_spatial_question(processor, model, image, obj1, obj2):\n",
    "    \"\"\"Ask VLM spatial question with multiple prompts\"\"\"\n",
    "    prompts = [\n",
    "        f\"Question: From the viewer's perspective, where is the {obj1}? Answer with one word: left, right, above, or below.\",\n",
    "        f\"From the viewer's point of view, the {obj1} is located:\",\n",
    "        f\"Relative to the viewer, the {obj1} is on the:\",\n",
    "        f\"From the camera's perspective, where is the {obj1}? (left/right/above/below) Answer with one word:\",\n",
    "        f\"Viewer's perspective â€“ position of {obj1}:\"\n",
    "    ]\n",
    "    \n",
    "    all_answers = []\n",
    "    all_raw = []\n",
    "    \n",
    "    for question in prompts:\n",
    "        inputs = processor(images=image, text=question, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=8, num_beams=3, do_sample=False)\n",
    "        \n",
    "        raw_answer = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        all_raw.append(f\"{question[:30]}... -> {raw_answer}\")\n",
    "        \n",
    "        normalized = normalize_spatial_answer(raw_answer)\n",
    "        if normalized != 'unknown':\n",
    "            all_answers.append(normalized)\n",
    "    \n",
    "    if all_answers:\n",
    "        most_common = Counter(all_answers).most_common(1)[0][0]\n",
    "        return most_common, \" | \".join(all_raw)\n",
    "    \n",
    "    return 'unknown', \" | \".join(all_raw)\n",
    "\n",
    "\n",
    "def evaluate_vlm_soft(vlm_relation, claimed_relation, ground_truth_label):\n",
    "    \"\"\"Soft evaluation: Compatible relations count as correct\"\"\"\n",
    "    if vlm_relation == 'unknown':\n",
    "        return None\n",
    "    \n",
    "    compatibility = get_relation_compatibility(vlm_relation, claimed_relation)\n",
    "    vlm_says_compatible = (compatibility >= 0.7)\n",
    "    \n",
    "    if ground_truth_label:\n",
    "        return vlm_says_compatible\n",
    "    else:\n",
    "        return not vlm_says_compatible\n",
    "\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n",
    "\n",
    "# ================================================================\n",
    "# MODEL LOADING\n",
    "# ================================================================\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load GroundingDINO and BLIP-2 models\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    grounding_proc = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    \n",
    "    # blip2_proc = processorprocessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    blip2_proc = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        grounding_model = grounding_model.cuda()\n",
    "        blip2_model = blip2_model.cuda()\n",
    "    \n",
    "    grounding_model.eval()\n",
    "    blip2_model.eval()\n",
    "    \n",
    "    print(\"âœ“ Models loaded\")\n",
    "    return grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION FUNCTION WITH MULTIPLE SIGNALS\n",
    "# ================================================================\n",
    "\n",
    "def evaluate_sample_vision_only_with_signals(\n",
    "    sample,\n",
    "    grounding_proc,\n",
    "    grounding_model,\n",
    "    vlm_proc,\n",
    "    vlm_model\n",
    "):\n",
    "    \"\"\"\n",
    "    Vision-only evaluation with MULTIPLE confidence signals.\n",
    "    The ML model will learn how to combine them.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    obj1 = sample['obj1']\n",
    "    obj2 = sample['obj2']\n",
    "    claimed_relation = sample['relation']\n",
    "    ground_truth_label = sample['label']\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 1: VLM Prediction (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    vlm_relation, vlm_raw = ask_vlm_spatial_question(\n",
    "        vlm_proc, vlm_model, image, obj1, obj2\n",
    "    )\n",
    "    \n",
    "    vlm_is_correct = evaluate_vlm_soft(vlm_relation, claimed_relation, ground_truth_label)\n",
    "    \n",
    "    if vlm_is_correct is None:\n",
    "        return None\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 2: Geometric Validation (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    detection_result = detect_objects_groundingdino(\n",
    "        grounding_proc, grounding_model, image, [obj1, obj2], score_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    obj1_box, obj1_score = get_best_detection_groundingdino(detection_result, obj1)\n",
    "    obj2_box, obj2_score = get_best_detection_groundingdino(detection_result, obj2)\n",
    "    \n",
    "    objects_detected = (obj1_box is not None and obj2_box is not None)\n",
    "    \n",
    "    if not objects_detected:\n",
    "        owl_relation = 'none'\n",
    "        geometric_confidence = 0.0\n",
    "        separation_confidence = 0.0\n",
    "        iou = 0.0\n",
    "    else:\n",
    "        owl_relation = compute_geometric_relation(obj1_box, obj2_box)\n",
    "        \n",
    "        if vlm_relation == 'unknown':\n",
    "            geometric_confidence = 0.0\n",
    "        else:\n",
    "            base_conf = validate_spatial_claim_with_coordinates(\n",
    "                vlm_relation, obj1_box, obj2_box\n",
    "            )\n",
    "            detection_quality = (obj1_score + obj2_score) / 2\n",
    "            quality_boost = 1 / (1 + np.exp(-10 * (detection_quality - 0.3)))\n",
    "            geometric_confidence = base_conf * (0.5 + 0.5 * quality_boost)\n",
    "        \n",
    "        # Box separation (IoU)\n",
    "        x1_min, y1_min, x1_max, y1_max = obj1_box\n",
    "        x2_min, y2_min, x2_max, y2_max = obj2_box\n",
    "        \n",
    "        inter_x_min = max(x1_min, x2_min)\n",
    "        inter_y_min = max(y1_min, y2_min)\n",
    "        inter_x_max = min(x1_max, x2_max)\n",
    "        inter_y_max = min(y1_max, y2_max)\n",
    "        \n",
    "        if inter_x_max > inter_x_min and inter_y_max > inter_y_min:\n",
    "            inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "        else:\n",
    "            inter_area = 0\n",
    "        \n",
    "        area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "        area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "        union_area = area1 + area2 - inter_area\n",
    "        \n",
    "        iou = inter_area / union_area if union_area > 0 else 0\n",
    "        separation_confidence = 1.0 - iou\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 3: Text Consistency (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    questions = [\n",
    "        f\"Where is the {obj1} relative to the {obj2}?\",\n",
    "        f\"Is the {obj1} to the left, right, above, or below the {obj2}?\",\n",
    "        f\"Position of {obj1}:\",\n",
    "        f\"The {obj1} is located:\",\n",
    "        f\"Spatial relation between {obj1} and {obj2}:\"\n",
    "    ]\n",
    "    \n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        inputs = vlm_proc(images=image, text=question, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = vlm_model.generate(**inputs, max_new_tokens=8, num_beams=1)\n",
    "        \n",
    "        answer = vlm_proc.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        normalized = normalize_spatial_answer(answer)\n",
    "        if normalized != 'unknown':\n",
    "            answers.append(normalized)\n",
    "    \n",
    "    if answers:\n",
    "        most_common_count = Counter(answers).most_common(1)[0][1]\n",
    "        text_consistency = most_common_count / len(answers)\n",
    "    else:\n",
    "        text_consistency = 0.0\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 4: VLM Token Probability (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    try:\n",
    "        question = f\"Where is the {obj1} relative to the {obj2}?\"\n",
    "        inputs = vlm_proc(images=image, text=question, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=8,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "                num_beams=1\n",
    "            )\n",
    "        \n",
    "        if len(outputs.scores) > 0:\n",
    "            token_probs = torch.softmax(outputs.scores[0][0], dim=-1)\n",
    "            vlm_token_confidence = token_probs.max().item()\n",
    "        else:\n",
    "            vlm_token_confidence = 0.5\n",
    "    except:\n",
    "        vlm_token_confidence = 0.5\n",
    "    \n",
    "    # ================================================================\n",
    "    # SIGNAL 5: Relation Compatibility (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    relation_compatibility = get_relation_compatibility(vlm_relation, owl_relation)\n",
    "    \n",
    "    # ================================================================\n",
    "    # NEW SIGNAL 6: CLIP Spatial Features (ADD THIS SECTION)\n",
    "    # ================================================================\n",
    "    clip_features = extract_clip_spatial_features(\n",
    "        image, obj1, obj2, claimed_relation\n",
    "    )\n",
    "    \n",
    "    # ================================================================\n",
    "    # AGGREGATE (YOUR EXISTING CODE - UNCHANGED)\n",
    "    # ================================================================\n",
    "    base_confidence = (\n",
    "        0.35 * geometric_confidence +\n",
    "        0.25 * text_consistency +\n",
    "        0.20 * vlm_token_confidence +\n",
    "        0.10 * separation_confidence +\n",
    "        0.10 * (min(obj1_score, obj2_score) if objects_detected else 0)\n",
    "    )\n",
    "    \n",
    "    # Determine reason\n",
    "    if not objects_detected:\n",
    "        confidence_reason = \"NO_DETECTION\"\n",
    "    elif vlm_relation == 'unknown':\n",
    "        confidence_reason = \"VLM_NO_ANSWER\"\n",
    "    elif relation_compatibility >= 0.7:\n",
    "        confidence_reason = \"GEOMETRIC_MATCH\"\n",
    "    else:\n",
    "        confidence_reason = \"GEOMETRIC_MISMATCH\"\n",
    "    \n",
    "    # ================================================================\n",
    "    # BUILD RESULT (MODIFIED - add CLIP features)\n",
    "    # ================================================================\n",
    "    result = {\n",
    "        'image_path': str(sample['image_path']),\n",
    "        'obj1': str(obj1),\n",
    "        'obj2': str(obj2),\n",
    "        'claimed_relation': str(claimed_relation),\n",
    "        'ground_truth_label': bool(ground_truth_label),\n",
    "        \n",
    "        'vlm_relation': str(vlm_relation),\n",
    "        'owl_relation': str(owl_relation),\n",
    "        'vlm_is_correct': bool(vlm_is_correct),\n",
    "        \n",
    "        # Your existing signals\n",
    "        'geometric_confidence': float(geometric_confidence),\n",
    "        'text_consistency': float(text_consistency),\n",
    "        'vlm_token_confidence': float(vlm_token_confidence),\n",
    "        'separation_confidence': float(separation_confidence),\n",
    "        'relation_compatibility': float(relation_compatibility),\n",
    "        'obj1_score': float(obj1_score) if obj1_score else 0.0,\n",
    "        'obj2_score': float(obj2_score) if obj2_score else 0.0,\n",
    "        'detection_quality': float((obj1_score + obj2_score) / 2) if objects_detected else 0.0,\n",
    "        'objects_detected': int(objects_detected),\n",
    "        'iou': float(iou),\n",
    "        \n",
    "        # Legacy fields\n",
    "        'confidence': float(base_confidence),\n",
    "        'confidence_reason': str(confidence_reason),\n",
    "        'trusts_vlm': bool(base_confidence >= 0.5),\n",
    "        'correct': bool((base_confidence >= 0.5) == vlm_is_correct)\n",
    "    }\n",
    "    \n",
    "    # ADD CLIP features to result\n",
    "    result.update(clip_features)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ================================================================\n",
    "# DATASET LOADING\n",
    "# ================================================================\n",
    "\n",
    "def load_vsr_dataset(jsonl_path, images_dir):\n",
    "    \"\"\"Load VSR dataset\"\"\"\n",
    "    samples = []\n",
    "    valid_2d_relations = {'left', 'right', 'above', 'below', 'near'}\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            obj1 = item.get('subj', '')\n",
    "            obj2 = item.get('obj', '')\n",
    "            \n",
    "            relation = item.get('relation', 'near')\n",
    "            relation_map = {\n",
    "                'next to': 'near', 'near': 'near',\n",
    "                'left of': 'left', 'right of': 'right',\n",
    "                'above': 'above', 'below': 'below',\n",
    "                'on': 'above', 'under': 'below'\n",
    "            }\n",
    "            relation = relation_map.get(relation.lower(), relation.lower())\n",
    "            \n",
    "            if relation not in valid_2d_relations:\n",
    "                continue\n",
    "            \n",
    "            image_name = item['image']\n",
    "            image_link = item.get('image_link', '')\n",
    "            split = 'val2017' if 'val2017' in image_link else 'train2017'\n",
    "            \n",
    "            samples.append({\n",
    "                'image_path': Path(images_dir) / split / image_name,\n",
    "                'caption': item.get('caption', ''),\n",
    "                'obj1': obj1,\n",
    "                'obj2': obj2,\n",
    "                'relation': relation,\n",
    "                'label': bool(item.get('label', 1))\n",
    "            })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# ================================================================\n",
    "# ML CONFIDENCE PREDICTOR\n",
    "# ================================================================\n",
    "from xgboost import XGBClassifier\n",
    "def train_ml_confidence_predictor(results_list, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Train Gradient Boosting model to predict VLM correctness.\n",
    "    The model learns optimal combination of all signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING ML CONFIDENCE PREDICTOR\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define features (all our signals)\n",
    "    feature_cols = [\n",
    "        'geometric_confidence',\n",
    "        'text_consistency',\n",
    "        'vlm_token_confidence',\n",
    "        'separation_confidence',\n",
    "        'relation_compatibility',\n",
    "        'obj1_score',\n",
    "        'obj2_score',\n",
    "        'detection_quality',\n",
    "        'objects_detected',\n",
    "        'iou',\n",
    "        'clip_layer5_lr_sep',\n",
    "        'clip_layer5_tb_sep',\n",
    "        'clip_spatial_preservation',\n",
    "        'clip_axis_alignment',\n",
    "        'clip_contrastive_score',\n",
    "        'clip_contrastive_margin'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_cols].fillna(0).values\n",
    "    y = df['vlm_is_correct'].astype(int).values\n",
    "    \n",
    "    print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "    print(f\"Samples:  {len(X)}\")\n",
    "    print(f\"Positive: {y.sum()} ({y.mean()*100:.1f}%)\")\n",
    "    print(f\"Negative: {len(y) - y.sum()} ({(1-y.mean())*100:.1f}%)\")\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain: {len(X_train)} | Validation: {len(X_val)}\")\n",
    "    \n",
    "    # Train Gradient Boosting\n",
    "    print(\"\\nTraining Gradient Boosting Classifier...\")\n",
    "    \"\"\"\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,      # Reduce from 200\n",
    "        learning_rate=0.05,    # Reduce from 0.1\n",
    "        max_depth=3,           # Reduce from 5\n",
    "        min_samples_split=20,  # Increase from 10\n",
    "        min_samples_leaf=10,   # Increase from 5\n",
    "        subsample=0.7,         # Reduce from 0.8\n",
    "        random_state=42\n",
    "    )\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,        # L1 regularization\n",
    "        reg_lambda=1.0,       # L2 regularization\n",
    "        random_state=42\n",
    "    )\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    train_auroc = roc_auc_score(y_train, train_probs)\n",
    "    val_auroc = roc_auc_score(y_val, val_probs)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training Complete\")\n",
    "    print(f\"  Train AUROC: {train_auroc:.3f}\")\n",
    "    print(f\"  Val AUROC:   {val_auroc:.3f}\")\n",
    "    \n",
    "    # Feature importances\n",
    "    print(f\"\\nLearned Feature Importances:\")\n",
    "    print(f\"{'Feature':<30} | {'Importance'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    \n",
    "    for idx in sorted_idx:\n",
    "        feat_name = feature_cols[idx]\n",
    "        imp = importances[idx]\n",
    "        bar = 'â–ˆ' * int(imp * 50)\n",
    "        print(f\"{feat_name:<30} | {imp:.4f} {bar}\")\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n",
    "    youden_index = tpr - fpr\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"\\nOptimal Threshold: {best_threshold:.3f}\")\n",
    "    \n",
    "    # Evaluate at threshold\n",
    "    preds = (val_probs >= best_threshold)\n",
    "    tp = ((preds) & (y_val == 1)).sum()\n",
    "    fp = ((preds) & (y_val == 0)).sum()\n",
    "    fn = ((~preds) & (y_val == 1)).sum()\n",
    "    tn = ((~preds) & (y_val == 0)).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    \n",
    "    return model, best_threshold, feature_cols\n",
    "\n",
    "\n",
    "def apply_ml_confidence(results_list, model, threshold, feature_cols):\n",
    "    \"\"\"Apply trained ML model to get learned confidence scores\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Extract features\n",
    "    X = df[feature_cols].fillna(0).values\n",
    "    \n",
    "    # Get predictions\n",
    "    ml_confidence = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    df['ml_confidence'] = ml_confidence\n",
    "    df['ml_trusts_vlm'] = (ml_confidence >= threshold)\n",
    "    df['ml_correct'] = (df['ml_trusts_vlm'] == df['vlm_is_correct'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================\n",
    "# MAIN EXECUTION\n",
    "# ================================================================\n",
    "\n",
    "# Load models\n",
    "grounding_proc, grounding_model, blip2_proc, blip2_model = load_models()\n",
    "\n",
    "# Load VSR dataset\n",
    "VSR_PATH = \"vsr_dataset/train.jsonl\"\n",
    "VSR_IMAGES = \"vsr_dataset/images/\"\n",
    "print(f\"\\nLoading dataset from {VSR_PATH}...\")\n",
    "vsr_samples = load_vsr_dataset(VSR_PATH, VSR_IMAGES)\n",
    "print(f\"âœ“ Loaded {len(vsr_samples)} samples\")\n",
    "\n",
    "TEST_SIZE = 500\n",
    "if TEST_SIZE:\n",
    "    vsr_samples = vsr_samples[:TEST_SIZE]\n",
    "    print(f\"Using {TEST_SIZE} samples\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISION-ONLY CONFIDENCE ESTIMATION (GroundingDINO + ML)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for sample in tqdm(vsr_samples, desc=\"Evaluating\"):\n",
    "    result = evaluate_sample_vision_only_with_signals(\n",
    "        sample, grounding_proc, grounding_model, blip2_proc, blip2_model\n",
    "    )\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\nâœ“ Completed: {len(results)} samples\")\n",
    "\n",
    "# Save results\n",
    "with open('results_vision_only_groundingdino.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved\")\n",
    "\n",
    "# ================================================================\n",
    "# BASELINE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RESULTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nTotal Samples: {len(df)}\")\n",
    "\n",
    "# VLM Baseline\n",
    "vlm_acc = df['vlm_is_correct'].mean()\n",
    "print(f\"\\nVLM Baseline Accuracy: {vlm_acc:.3f}\")\n",
    "\n",
    "# Vision-Only Method\n",
    "our_acc = df['correct'].mean()\n",
    "try:\n",
    "    our_auroc = roc_auc_score(df['vlm_is_correct'], df['confidence'])\n",
    "except:\n",
    "    our_auroc = 0.5\n",
    "\n",
    "print(f\"\\nBaseline Geometric Confidence:\")\n",
    "print(f\"  Accuracy:  {our_acc:.3f}\")\n",
    "print(f\"  AUROC:     {our_auroc:.3f}\")\n",
    "\n",
    "# Detection statistics\n",
    "detected = df['objects_detected'].sum()\n",
    "print(f\"\\n  Objects Detected: {detected}/{len(df)} ({detected/len(df):.1%})\")\n",
    "\n",
    "avg_obj1_score = df[df['obj1_score'] > 0]['obj1_score'].mean()\n",
    "avg_obj2_score = df[df['obj2_score'] > 0]['obj2_score'].mean()\n",
    "print(f\"  Avg Detection Quality: obj1={avg_obj1_score:.3f}, obj2={avg_obj2_score:.3f}\")\n",
    "\n",
    "# Confidence reasons\n",
    "print(f\"\\nConfidence Reasoning:\")\n",
    "for reason, count in df['confidence_reason'].value_counts().items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {reason:<25}: {count:>4} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(df['vlm_is_correct'], df['trusts_vlm'])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nBaseline Confusion Matrix:\")\n",
    "print(f\"  TP: {tp:3d} ({tp/len(df)*100:5.1f}%) - Correctly trusted VLM\")\n",
    "print(f\"  TN: {tn:3d} ({tn/len(df)*100:5.1f}%) - Correctly rejected VLM\")\n",
    "print(f\"  FP: {fp:3d} ({fp/len(df)*100:5.1f}%) - Wrongly trusted VLM\")\n",
    "print(f\"  FN: {fn:3d} ({fn/len(df)*100:5.1f}%) - Wrongly rejected VLM\")\n",
    "\n",
    "# Precision/Recall\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\n  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Coverage @ Risk\n",
    "print(f\"\\nBaseline Coverage @ Risk:\")\n",
    "print(f\"{'Risk':<10} | {'Target Acc':<12} | {'Coverage'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "risk_levels = [0.10, 0.20, 0.30, 0.40, 0.50]\n",
    "\n",
    "for risk in risk_levels:\n",
    "    target_acc = 1.0 - risk\n",
    "    df_sorted = df.sort_values('confidence', ascending=False)\n",
    "    \n",
    "    best_coverage = 0.0\n",
    "    for i in range(1, len(df_sorted) + 1):\n",
    "        subset = df_sorted.iloc[:i]\n",
    "        acc = subset['vlm_is_correct'].mean()\n",
    "        if acc >= target_acc:\n",
    "            best_coverage = i / len(df_sorted)\n",
    "    \n",
    "    print(f\"{risk*100:>4.0f}%{' ':<5} | {target_acc*100:>6.0f}%{' ':<5} | {best_coverage:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ Baseline Evaluation Complete\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# TRAIN ML CONFIDENCE PREDICTOR\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ML-BASED CONFIDENCE LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train ML model\n",
    "ml_model, ml_threshold, feature_cols = train_ml_confidence_predictor(\n",
    "    results,\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "# Apply ML model\n",
    "df_ml = apply_ml_confidence(results, ml_model, ml_threshold, feature_cols)\n",
    "\n",
    "# Save ML results\n",
    "df_ml.to_json('results_ml_confidence.json', orient='records', indent=2)\n",
    "joblib.dump(ml_model, 'ml_confidence_model.pkl')\n",
    "\n",
    "print(\"\\nâœ“ ML model and predictions saved\")\n",
    "\n",
    "# ================================================================\n",
    "# ML RESULTS COMPARISON\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE vs ML-LEARNED CONFIDENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_raw = pd.DataFrame(results)\n",
    "\n",
    "# Compare AUROC\n",
    "raw_auroc = roc_auc_score(df_raw['vlm_is_correct'], df_raw['confidence'])\n",
    "ml_auroc = roc_auc_score(df_ml['vlm_is_correct'], df_ml['ml_confidence'])\n",
    "\n",
    "raw_acc = df_raw['correct'].mean()\n",
    "ml_acc = df_ml['ml_correct'].mean()\n",
    "\n",
    "print(f\"\\n{'Metric':<30} | {'Baseline':<12} | {'ML-Learned':<12} | {'Improvement'}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'AUROC':<30} | {raw_auroc:<12.3f} | {ml_auroc:<12.3f} | {(ml_auroc-raw_auroc)/raw_auroc*100:>+6.1f}%\")\n",
    "print(f\"{'Accuracy':<30} | {raw_acc:<12.3f} | {ml_acc:<12.3f} | {(ml_acc-raw_acc)/raw_acc*100:>+6.1f}%\")\n",
    "\n",
    "# Confusion matrices\n",
    "print(f\"\\nBaseline Confusion Matrix:\")\n",
    "cm_raw = confusion_matrix(df_raw['vlm_is_correct'], df_raw['trusts_vlm'])\n",
    "tn, fp, fn, tp = cm_raw.ravel()\n",
    "print(f\"  TP: {tp:3d} ({tp/len(df_raw)*100:5.1f}%) | FP: {fp:3d} ({fp/len(df_raw)*100:5.1f}%)\")\n",
    "print(f\"  FN: {fn:3d} ({fn/len(df_raw)*100:5.1f}%) | TN: {tn:3d} ({tn/len(df_raw)*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nML-Learned Confusion Matrix:\")\n",
    "cm_ml = confusion_matrix(df_ml['vlm_is_correct'], df_ml['ml_trusts_vlm'])\n",
    "tn, fp, fn, tp = cm_ml.ravel()\n",
    "print(f\"  TP: {tp:3d} ({tp/len(df_ml)*100:5.1f}%) | FP: {fp:3d} ({fp/len(df_ml)*100:5.1f}%)\")\n",
    "print(f\"  FN: {fn:3d} ({fn/len(df_ml)*100:5.1f}%) | TN: {tn:3d} ({tn/len(df_ml)*100:5.1f}%)\")\n",
    "\n",
    "# ML metrics\n",
    "tp = ((df_ml['ml_trusts_vlm'] == True) & (df_ml['vlm_is_correct'] == True)).sum()\n",
    "fp = ((df_ml['ml_trusts_vlm'] == True) & (df_ml['vlm_is_correct'] == False)).sum()\n",
    "fn = ((df_ml['ml_trusts_vlm'] == False) & (df_ml['vlm_is_correct'] == True)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nML-Learned Metrics:\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Coverage @ Risk comparison\n",
    "print(f\"\\nCoverage @ Risk Comparison:\")\n",
    "print(f\"{'Risk':<10} | {'Target Acc':<12} | {'Baseline':<12} | {'ML-Learned':<12} | {'Improvement'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for risk in risk_levels:\n",
    "    target_acc = 1.0 - risk\n",
    "    \n",
    "    # Baseline coverage\n",
    "    df_sorted_raw = df_raw.sort_values('confidence', ascending=False)\n",
    "    best_cov_raw = 0.0\n",
    "    for i in range(1, len(df_sorted_raw) + 1):\n",
    "        if df_sorted_raw.iloc[:i]['vlm_is_correct'].mean() >= target_acc:\n",
    "            best_cov_raw = i / len(df_sorted_raw)\n",
    "    \n",
    "    # ML coverage\n",
    "    df_sorted_ml = df_ml.sort_values('ml_confidence', ascending=False)\n",
    "    best_cov_ml = 0.0\n",
    "    for i in range(1, len(df_sorted_ml) + 1):\n",
    "        if df_sorted_ml.iloc[:i]['vlm_is_correct'].mean() >= target_acc:\n",
    "            best_cov_ml = i / len(df_sorted_ml)\n",
    "    \n",
    "    improvement = (best_cov_ml - best_cov_raw) / best_cov_raw * 100 if best_cov_raw > 0 else float('inf')\n",
    "    \n",
    "    print(f\"{risk*100:>4.0f}%{' ':<5} | {target_acc*100:>6.0f}%{' ':<5} | \"\n",
    "          f\"{best_cov_raw:<12.3f} | {best_cov_ml:<12.3f} | {improvement:>+7.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ ML CONFIDENCE LEARNING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0be8b3-4f7d-44d3-be96-772d29deb55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
